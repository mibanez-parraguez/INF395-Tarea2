{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. *Encoder-Decoder* sobre Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elige el inglés al español ya que es mas fácil comprobar resultados de forma natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "The available CPU/GPU devices on your system\n",
      "====================================================================================================\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17451588044000884234\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3150367948\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3576444456178468212\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# Set the environment variables\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Double check that you have the correct devices visible to TF\n",
    "print(\"{0}\\nThe available CPU/GPU devices on your system\\n{0}\".format('=' * 100))\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# Different options to start with GPU or CPU. I am using the CPU. Can be changed from the below options\n",
    "# with tf.device('/cpu:0'):\n",
    "with tf.device('/gpu:0'):\n",
    "# with tf.Graph().as_default():\n",
    "    config = tf.ConfigProto(device_count={'GPU': 1}, log_device_placement=False,\n",
    "                            allow_soft_placement=True)\n",
    "    # allocate only as much GPU memory based on runtime allocations\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Visualice los datos ¿Qué es la entrada y qué es la salida? Comente sobre los múltiples significados/sinónimos que puede tener una palabra al ser traducida y cómo propondría arreglar eso. *se espera que pueda implementarlo*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiples significados/sinónimos para una palabra va a confundir a la red. Una forma sencilla es simplemente eliminar las filas duplicadas y solo dejar la primera ocurrencia de la palabra, sin embargo algunas tienen símbolos que las diferencian que mas adelante serán eliminados por lo que el paso de eliminar frases duplicadas se hará mas adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246752\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corran!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corra!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corred!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Corred.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Who?</td>\n",
       "      <td>¿Quién?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>¡Órale!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>¡Fuego!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>¡Incendio!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>¡Disparad!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Help!</td>\n",
       "      <td>¡Ayuda!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Help!</td>\n",
       "      <td>¡Socorro! ¡Auxilio!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Help!</td>\n",
       "      <td>¡Auxilio!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jump!</td>\n",
       "      <td>¡Salta!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>Salte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>¡Parad!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>¡Para!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>¡Pare!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>¡Espera!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wait.</td>\n",
       "      <td>Esperen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Go on.</td>\n",
       "      <td>Continúa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Go on.</td>\n",
       "      <td>Continúe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I ran.</td>\n",
       "      <td>Corrí.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I ran.</td>\n",
       "      <td>Corría.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I try.</td>\n",
       "      <td>Lo intento.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I won!</td>\n",
       "      <td>¡He ganado!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Oh no!</td>\n",
       "      <td>¡Oh, no!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Relax.</td>\n",
       "      <td>Tomátelo con soda.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Shoot!</td>\n",
       "      <td>¡Fuego!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Shoot!</td>\n",
       "      <td>¡Disparad!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Shoot!</td>\n",
       "      <td>¡Disparen!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Shoot!</td>\n",
       "      <td>¡Dispara!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Shoot!</td>\n",
       "      <td>¡Dispará!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Shoot!</td>\n",
       "      <td>¡Dispare!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Smile.</td>\n",
       "      <td>Sonríe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Attack!</td>\n",
       "      <td>¡Al ataque!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Attack!</td>\n",
       "      <td>¡Atacad!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Attack!</td>\n",
       "      <td>¡Ataque!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Attack!</td>\n",
       "      <td>¡Ataquen!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Attack!</td>\n",
       "      <td>¡Ataca!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Get up.</td>\n",
       "      <td>Levanta.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Ve ahora mismo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Id ahora mismo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Vaya ahora mismo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Source               Target\n",
       "0       Go.                  Ve.\n",
       "1       Go.                Vete.\n",
       "2       Go.                Vaya.\n",
       "3       Go.              Váyase.\n",
       "4       Hi.                Hola.\n",
       "5      Run!              ¡Corre!\n",
       "6      Run!             ¡Corran!\n",
       "7      Run!              ¡Corra!\n",
       "8      Run!             ¡Corred!\n",
       "9      Run.              Corred.\n",
       "10     Who?              ¿Quién?\n",
       "11     Wow!              ¡Órale!\n",
       "12    Fire!              ¡Fuego!\n",
       "13    Fire!           ¡Incendio!\n",
       "14    Fire!           ¡Disparad!\n",
       "15    Help!              ¡Ayuda!\n",
       "16    Help!  ¡Socorro! ¡Auxilio!\n",
       "17    Help!            ¡Auxilio!\n",
       "18    Jump!              ¡Salta!\n",
       "19    Jump.               Salte.\n",
       "20    Stop!              ¡Parad!\n",
       "21    Stop!               ¡Para!\n",
       "22    Stop!               ¡Pare!\n",
       "23    Wait!             ¡Espera!\n",
       "24    Wait.             Esperen.\n",
       "25   Go on.            Continúa.\n",
       "26   Go on.            Continúe.\n",
       "27   Hello!                Hola.\n",
       "28   I ran.               Corrí.\n",
       "29   I ran.              Corría.\n",
       "30   I try.          Lo intento.\n",
       "31   I won!          ¡He ganado!\n",
       "32   Oh no!             ¡Oh, no!\n",
       "33   Relax.   Tomátelo con soda.\n",
       "34   Shoot!              ¡Fuego!\n",
       "35   Shoot!           ¡Disparad!\n",
       "36   Shoot!           ¡Disparen!\n",
       "37   Shoot!            ¡Dispara!\n",
       "38   Shoot!            ¡Dispará!\n",
       "39   Shoot!            ¡Dispare!\n",
       "40   Smile.              Sonríe.\n",
       "41  Attack!          ¡Al ataque!\n",
       "42  Attack!             ¡Atacad!\n",
       "43  Attack!             ¡Ataque!\n",
       "44  Attack!            ¡Ataquen!\n",
       "45  Attack!              ¡Ataca!\n",
       "46  Get up.             Levanta.\n",
       "47  Go now.      Ve ahora mismo.\n",
       "48  Go now.      Id ahora mismo.\n",
       "49  Go now.    Vaya ahora mismo."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"data/spa.txt\", sep=\"\\t\", names=[\"Source\",\"Target\"])\n",
    "print(df.size)\n",
    "# df[9500:9550]\n",
    "# df[6200:6250]\n",
    "df[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Realice un pre-procesamiento a los textos como se acostumbra para eliminar símbolos inecesarios u otras cosas que estime conveniente, comente sobre la importancia de éste paso. Además de ésto deberá agregar un símbolo al final de la sentencia target para indicar un \"alto\" cuando la red neuronal necesite aprender a generar una sentencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso se eliminan símbolos, números y puntuaciones, además con el texto simplificado es mas fácil detectar frases duplicadas como \"Go now\", de forma que cada frase de source ahora tiene una frase de target único, con esto se espera que la red no confunda palabras iguales en distintos contextos. Con las frases limpias ahora es posible crear un diccionario evitando palabras duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "table = str.maketrans('', '', string.punctuation) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmaDict_es = {}\n",
    "with open('data/lemmatization-es.txt', 'rb') as f:\n",
    "    data = f.read().decode('utf8').replace(u'\\r',u'').split(u'\\n')\n",
    "    data = [a.split(u'\\t') for a in data]\n",
    "    \n",
    "for a in data:\n",
    "    if len(a) > 1:\n",
    "        lemmaDict_es[a[1]] = a[0]\n",
    "\n",
    "def lemmatize(word):\n",
    "    return lemmaDict_es.get(word, word)\n",
    "\n",
    "def clean_text(text, where=None):\n",
    "    \"\"\" OJO: Sin eliminar el significado de las palabras.\"\"\"\n",
    "    text = text.lower()\n",
    "    tokenize_text = nltk.word_tokenize(text)\n",
    "    tokenize_text = [''.join(c for c in word if c.isalnum()) for word in tokenize_text]#eliminar puntuacion\n",
    "#     tokenize_text = [word.translate(table) for word in tokenize_text]#eliminar puntuacion# elimina palabras que empiezan con ! ej. !Ataque!\n",
    "    tokenize_text = [word for word in tokenize_text if word.isalpha()] #remove numbers\n",
    "#     tokenize_text = [lemmatizer.lemmatize(word) for word in tokenize_text]\n",
    "    if where ==\"target\":\n",
    "#         tokenize_text = [lemmatize(word) for word in tokenize_text]\n",
    "        tokenize_text = tokenize_text + [\"#end\"] \n",
    "    return tokenize_text\n",
    "texts_input = list(df['Source'].apply(clean_text))\n",
    "texts_output = list(df['Target'].apply(clean_text, where='target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "texts_input_no_duplicates = list()\n",
    "texts_output_no_duplicates = list()\n",
    "for index, sentence_list in enumerate(texts_input):\n",
    "    if str(sentence_list) not in dict:\n",
    "        texts_input_no_duplicates.append(sentence_list)\n",
    "        texts_output_no_duplicates.append(texts_output[index])\n",
    "        dict[str(sentence_list)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go'], ['go'], ['go'], ['go'], ['hi'], ['run'], ['run'], ['run'], ['run'], ['run'], ['who'], ['wow'], ['fire'], ['fire'], ['fire'], ['help'], ['help'], ['help'], ['jump'], ['jump'], ['stop'], ['stop'], ['stop'], ['wait'], ['wait'], ['go', 'on'], ['go', 'on'], ['hello'], ['i', 'ran'], ['i', 'ran'], ['i', 'try'], ['i', 'won'], ['oh', 'no'], ['relax'], ['shoot'], ['shoot'], ['shoot'], ['shoot'], ['shoot'], ['shoot'], ['smile'], ['attack'], ['attack'], ['attack'], ['attack'], ['attack'], ['get', 'up'], ['go', 'now'], ['go', 'now'], ['go', 'now']]\n"
     ]
    }
   ],
   "source": [
    "print(texts_input[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go'], ['hi'], ['run'], ['who'], ['wow'], ['fire'], ['help'], ['jump'], ['stop'], ['wait'], ['go', 'on'], ['hello'], ['i', 'ran'], ['i', 'try'], ['i', 'won'], ['oh', 'no'], ['relax'], ['shoot'], ['smile'], ['attack'], ['get', 'up'], ['go', 'now'], ['got', 'it'], ['he', 'ran'], ['hop', 'in'], ['hug', 'me'], ['i', 'fell'], ['i', 'know'], ['i', 'left'], ['i', 'lied'], ['i', 'lost'], ['i', 'quit'], ['i', 'sang'], ['i', 'work'], ['i', 'm'], ['i', 'm', 'up'], ['listen'], ['no', 'way'], ['really'], ['thanks'], ['try', 'it'], ['we', 'try'], ['we', 'won'], ['why', 'me'], ['ask', 'tom'], ['awesome'], ['be', 'calm'], ['be', 'cool'], ['be', 'fair'], ['be', 'good']]\n"
     ]
    }
   ],
   "source": [
    "print(texts_input_no_duplicates[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ve', '#end'], ['vete', '#end'], ['vaya', '#end'], ['váyase', '#end'], ['hola', '#end'], ['corre', '#end'], ['corran', '#end'], ['corra', '#end'], ['corred', '#end'], ['corred', '#end'], ['quién', '#end'], ['órale', '#end'], ['fuego', '#end'], ['incendio', '#end'], ['disparad', '#end'], ['ayuda', '#end'], ['socorro', 'auxilio', '#end'], ['auxilio', '#end'], ['salta', '#end'], ['salte', '#end'], ['parad', '#end'], ['para', '#end'], ['pare', '#end'], ['espera', '#end'], ['esperen', '#end'], ['continúa', '#end'], ['continúe', '#end'], ['hola', '#end'], ['corrí', '#end'], ['corría', '#end'], ['lo', 'intento', '#end'], ['he', 'ganado', '#end'], ['oh', 'no', '#end'], ['tomátelo', 'con', 'soda', '#end'], ['fuego', '#end'], ['disparad', '#end'], ['disparen', '#end'], ['dispara', '#end'], ['dispará', '#end'], ['dispare', '#end'], ['sonríe', '#end'], ['al', 'ataque', '#end'], ['atacad', '#end'], ['ataque', '#end'], ['ataquen', '#end'], ['ataca', '#end'], ['levanta', '#end'], ['ve', 'ahora', 'mismo', '#end'], ['id', 'ahora', 'mismo', '#end'], ['vaya', 'ahora', 'mismo', '#end']]\n"
     ]
    }
   ],
   "source": [
    "print(texts_output[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ve', '#end'], ['hola', '#end'], ['corre', '#end'], ['quién', '#end'], ['órale', '#end'], ['fuego', '#end'], ['ayuda', '#end'], ['salta', '#end'], ['parad', '#end'], ['espera', '#end'], ['continúa', '#end'], ['hola', '#end'], ['corrí', '#end'], ['lo', 'intento', '#end'], ['he', 'ganado', '#end'], ['oh', 'no', '#end'], ['tomátelo', 'con', 'soda', '#end'], ['fuego', '#end'], ['sonríe', '#end'], ['al', 'ataque', '#end'], ['levanta', '#end'], ['ve', 'ahora', 'mismo', '#end'], ['lo', 'tengo', '#end'], ['él', 'corrió', '#end'], ['métete', 'adentro', '#end'], ['abrázame', '#end'], ['me', 'caí', '#end'], ['yo', 'lo', 'sé', '#end'], ['salí', '#end'], ['mentí', '#end'], ['perdí', '#end'], ['dimito', '#end'], ['canté', '#end'], ['estoy', 'trabajando', '#end'], ['tengo', 'diecinueve', '#end'], ['estoy', 'levantado', '#end'], ['escucha', '#end'], ['no', 'puede', 'ser', '#end'], ['en', 'serio', '#end'], ['gracias', '#end'], ['pruébalo', '#end'], ['lo', 'procuramos', '#end'], ['ganamos', '#end'], ['por', 'qué', 'yo', '#end'], ['pregúntale', 'a', 'tom', '#end'], ['órale', '#end'], ['mantente', 'en', 'calma', '#end'], ['estate', 'tranquilo', '#end'], ['sé', 'justo', '#end'], ['sé', 'bueno', '#end']]\n"
     ]
    }
   ],
   "source": [
    "print(texts_output_no_duplicates[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un conjunto de validación y de pruebas fijos de Nexp=10000 datos ¿Cuántos datos quedan para entrenar? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento X: 85107\n",
      "Datos de entrenamiento Y: 85107\n",
      "Datos de validacion X: 10000\n",
      "Datos de validacion Y: 10000\n",
      "Datos de test X: 10000\n",
      "Datos de test Y: 10000\n",
      "Frases de source: \n",
      "[['tom', 'introduced', 'me', 'to', 'his', 'roommate'], ['it', 's', 'really', 'not', 'as', 'hard', 'to', 'do', 'as', 'you', 'think', 'it', 'is'], ['i', 'could', 'nt', 'stand', 'the', 'itching'], ['tom', 'brought', 'mary', 'to', 'the', 'station'], ['was', 'it', 'all', 'an', 'illusion'], ['tom', 'is', 'good', 'at', 'skiing'], ['i', 'like', 'the', 'beach'], ['he', 's', 'a', 'strange', 'guy'], ['are', 'nt', 'you', 'anxious'], ['how', 'did', 'that', 'happen', 'again'], ['shall', 'i', 'begin'], ['i', 'think', 'we', 'should', 'lower', 'the', 'price'], ['he', 'was', 'sentenced', 'to', 'prison'], ['he', 'touched', 'me', 'on', 'the', 'cheek'], ['the', 'wound', 'has', 'not', 'healed', 'yet']]\n",
      "Frases de target: \n",
      "[['tom', 'me', 'presentó', 'a', 'su', 'compañero', 'de', 'habitación', '#end'], ['si', 'no', 'es', 'tan', 'difícil', 'como', 'tú', 'piensas', '#end'], ['no', 'podía', 'soportar', 'la', 'picazón', '#end'], ['tom', 'llevó', 'a', 'mary', 'a', 'la', 'estación', '#end'], ['todo', 'fue', 'una', 'ilusión', '#end'], ['tom', 'es', 'bueno', 'esquiando', '#end'], ['me', 'gusta', 'la', 'playa', '#end'], ['él', 'es', 'un', 'tipo', 'extraño', '#end'], ['no', 'estás', 'ansioso', '#end'], ['cómo', 'permití', 'que', 'pasara', 'otra', 'vez', '#end'], ['empiezo', '#end'], ['creo', 'que', 'deberíamos', 'bajar', 'el', 'precio', '#end'], ['él', 'fue', 'sentenciado', 'a', 'prisión', '#end'], ['él', 'me', 'tocó', 'en', 'la', 'mejilla', '#end'], ['la', 'herida', 'no', 'se', 'ha', 'curado', 'aún', '#end']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "N_exp = 10000\n",
    "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(texts_input_no_duplicates, texts_output_no_duplicates,\n",
    "                                                            test_size=N_exp, random_state=22)\n",
    "X_train_l, X_val_l, Y_train_l, Y_val_l = train_test_split(X_train_l, Y_train_l, \n",
    "                                                          test_size=N_exp, random_state=22)\n",
    "print(\"Datos de entrenamiento X: {}\".format(len(X_train_l)))\n",
    "print(\"Datos de entrenamiento Y: {}\".format(len(Y_train_l)))\n",
    "print(\"Datos de validacion X: {}\".format((len(X_val_l))))\n",
    "print(\"Datos de validacion Y: {}\".format((len(Y_val_l))))\n",
    "print(\"Datos de test X: {}\".format((len(X_test_l))))\n",
    "print(\"Datos de test Y: {}\".format((len(Y_test_l))))\n",
    "print(\"Frases de source: \\n{}\".format(X_train_l[:15]))\n",
    "print(\"Frases de target: \\n{}\".format(Y_train_l[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "en_l = []\n",
    "es_l = []\n",
    "for i in X_train_l:\n",
    "    en_l.append(len(i))\n",
    "for i in Y_train_l:\n",
    "    es_l.append(len(i))\n",
    "df_lengths = pd.DataFrame({'en':en_l, 'es':es_l})\n",
    "df_lengths.hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Genere un vocabulario, desde el conjunto de entrenamiento, sobre las palabras a recibir y generar en la traducción, esto es codificarlas a un valor entero que servirá para que la red las vea en una representación útil a procesar, comience desde el 1 debido a que el cero será utilizado más adelante. Para reducir el vocabulario considere las palabras que aparecen un mínimo de min_count veces en todo los datos, se aconseja un valor de 3. Comente sobre la importancia de ésto al reducir el vocabulario ¿De qué tamaño es el vocabulario de entrada y salida? ¿La diferencia de ésto podría ser un factor importante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de source: 6102\n",
      "Diccionario de target: 9235\n",
      "Primeras 15 entradas de source: \n",
      "[(1, 'tom'), (2, 'introduced'), (3, 'me'), (4, 'to'), (5, 'his'), (6, 'roommate'), (7, 'it'), (8, 's'), (9, 'really'), (10, 'not'), (11, 'as'), (12, 'hard'), (13, 'do'), (14, 'you'), (15, 'think')]\n",
      "Primeras 15 entradas de target: \n",
      "[(1, 'tom'), (2, 'me'), (3, 'presentó'), (4, 'a'), (5, 'su'), (6, 'compañero'), (7, 'de'), (8, 'habitación'), (9, '#end'), (10, 'si'), (11, 'no'), (12, 'es'), (13, 'tan'), (14, 'difícil'), (15, 'como')]\n"
     ]
    }
   ],
   "source": [
    "def create_vocab(texts, min_count=1):\n",
    "    count_vocab = {}\n",
    "    for sentence in texts:\n",
    "        for word in sentence:\n",
    "            if word not in count_vocab:\n",
    "                count_vocab[word] = 1\n",
    "            else:\n",
    "                count_vocab[word] += 1\n",
    "    return [word for word,count in count_vocab.items() if count >= min_count]\n",
    "vocab_source = create_vocab(X_train_l, min_count=3)\n",
    "# vocab_source = create_vocab(texts_input, min_count=3)\n",
    "word2idx_s = {w: i+1 for i, w in enumerate(vocab_source)} #index (i+1) start from 1,2,3,...\n",
    "idx2word_s = {i+1: w for i, w in enumerate(vocab_source)}\n",
    "n_words_s = len(vocab_source)\n",
    "vocab_target = create_vocab(Y_train_l, min_count=3)\n",
    "# vocab_target = create_vocab(texts_output, min_count=3)\n",
    "word2idx_t = {w: i+1 for i, w in enumerate(vocab_target)}  #Converting text to numbers\n",
    "idx2word_t = {i+1: w for i, w in enumerate(vocab_target)} #Converting number to text\n",
    "n_words_t = len(vocab_target)\n",
    "print(\"Diccionario de source: {}\".format(n_words_s))\n",
    "print(\"Diccionario de target: {}\".format(n_words_t))\n",
    "print(\"Primeras 15 entradas de source: \\n{}\".format(list(idx2word_s.items())[:15]))\n",
    "print(\"Primeras 15 entradas de target: \\n{}\".format(list(idx2word_t.items())[:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia del tamaño del vocabulario implica que existen palabras en target que no tienen una traduccion directa en source, o que algunas palabras en source tienen múltiples significados en target, lo que ya se observo en la parte a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora codifique las palabras a los números indexados con el vocabulario. Recuerde que si una palabra en los otros conjuntos, o en el mismo de entrenamiento, no aparece en el vocabulario no se podrá generar una codificación, por lo que será ignorada ¿Cómo se podría evitar ésto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tom', 'introduced', 'me', 'to', 'his', 'roommate'], ['it', 's', 'really', 'not', 'as', 'hard', 'to', 'do', 'as', 'you', 'think', 'it', 'is'], ['i', 'could', 'nt', 'stand', 'the', 'itching'], ['tom', 'brought', 'mary', 'to', 'the', 'station'], ['was', 'it', 'all', 'an', 'illusion'], ['tom', 'is', 'good', 'at', 'skiing'], ['i', 'like', 'the', 'beach'], ['he', 's', 'a', 'strange', 'guy'], ['are', 'nt', 'you', 'anxious'], ['how', 'did', 'that', 'happen', 'again'], ['shall', 'i', 'begin'], ['i', 'think', 'we', 'should', 'lower', 'the', 'price'], ['he', 'was', 'sentenced', 'to', 'prison'], ['he', 'touched', 'me', 'on', 'the', 'cheek'], ['the', 'wound', 'has', 'not', 'healed', 'yet']]\n",
      "[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12, 4, 13, 11, 14, 15, 7, 16], [17, 18, 19, 20, 21], [1, 22, 23, 4, 21, 24], [25, 7, 26, 27, 28], [1, 16, 29, 30, 31], [17, 32, 21, 33], [34, 8, 35, 36, 37], [38, 19, 14, 39], [40, 41, 42, 43, 44], [45, 17, 46], [17, 15, 47, 48, 49, 21, 50], [34, 25, 51, 4, 52], [34, 53, 3, 54, 21, 55], [21, 56, 57, 10, 58]]\n",
      "[['tom', 'me', 'presentó', 'a', 'su', 'compañero', 'de', 'habitación', '#end'], ['si', 'no', 'es', 'tan', 'difícil', 'como', 'tú', 'piensas', '#end'], ['no', 'podía', 'soportar', 'la', 'picazón', '#end'], ['tom', 'llevó', 'a', 'mary', 'a', 'la', 'estación', '#end'], ['todo', 'fue', 'una', 'ilusión', '#end'], ['tom', 'es', 'bueno', 'esquiando', '#end'], ['me', 'gusta', 'la', 'playa', '#end'], ['él', 'es', 'un', 'tipo', 'extraño', '#end'], ['no', 'estás', 'ansioso', '#end'], ['cómo', 'permití', 'que', 'pasara', 'otra', 'vez', '#end'], ['empiezo', '#end'], ['creo', 'que', 'deberíamos', 'bajar', 'el', 'precio', '#end'], ['él', 'fue', 'sentenciado', 'a', 'prisión', '#end'], ['él', 'me', 'tocó', 'en', 'la', 'mejilla', '#end'], ['la', 'herida', 'no', 'se', 'ha', 'curado', 'aún', '#end']]\n",
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 9], [11, 18, 19, 20, 9], [1, 21, 4, 22, 4, 20, 23, 9], [24, 25, 26, 27, 9], [1, 12, 28, 29, 9], [2, 30, 20, 31, 9], [32, 12, 33, 34, 35, 9], [11, 36, 37, 9], [38, 39, 40, 41, 42, 43, 9], [44, 9], [45, 40, 46, 47, 48, 49, 9], [32, 25, 50, 4, 51, 9], [32, 2, 52, 53, 20, 54, 9], [20, 55, 11, 56, 57, 58, 59, 9]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Source/input data \"\"\"\n",
    "dataX_train = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_train_l]\n",
    "dataX_valid = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_val_l]\n",
    "dataX_test = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_test_l]\n",
    "\"\"\" Target/output data \"\"\"\n",
    "dataY_train = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_train_l]\n",
    "dataY_valid = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_val_l]\n",
    "dataY_test = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_test_l]\n",
    "print(X_train_l[:15])\n",
    "print(dataX_train[:15])\n",
    "print(Y_train_l[:15])\n",
    "print(dataY_train[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Debido al largo variable de los textos de entrada y salida será necesario estandarizar ésto para poder trabajar de manera más cómoda en Keras, cada texto (entrada y salida) pueden tener distinto largo máximo. Comente sobre la decisión del tipo de padding, pre o post ¿Qué sucede al variar el largo máximo de instantes de tiempo para procesar en cada parte del modelo (entrada y salida)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dwarampudi & Reddy (2019). \"Effects of padding on LSTMs and CNNs\" [[1]](#refs)\n",
    "\n",
    "Pre y post padding tienen gran importancia en redes LSTM ya que tienen memoria, la cual se actualiza a medida que se introducen las palabras de las frases y en un estudio fue comprobado que tienen mejores resultado utilizando pre padding. En cambio en redes CNN pre y post padding no tienen relevancia ya que estas redes no tienen memoria, sino que se ajustan a los datos entregados tratando de encontrar patrones. Por lo que finalmente es mejor utilizar pre padding ya que en el peor de los casos no tiene beneficios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo max inp:  45\n",
      "Largo max out:  51\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\"\"\" INPUT DATA (Origin language) \"\"\"\n",
    "#TRAIN\n",
    "max_inp_length = max(map(len,dataX_train))\n",
    "max_inp_length_val = max(map(len,dataX_valid))\n",
    "max_inp_length_test = max(map(len,dataX_test))\n",
    "max_inp_len = max(max_inp_length,max_inp_length_val,max_inp_length_test)\n",
    "print(\"Largo max inp: \",max_inp_len)\n",
    "word2idx_s[\"*\"] = 0 #padding symbol\n",
    "idx2word_s[0] = \"*\"\n",
    "n_words_s += 1  \n",
    "X_train = sequence.pad_sequences(dataX_train, maxlen=max_inp_len, padding='pre', value=word2idx_s[\"*\"])\n",
    "#VALID\n",
    "# print(\"Largo max inp val: \",max_inp_length_val)\n",
    "X_val = sequence.pad_sequences(dataX_valid, maxlen=max_inp_len, padding='pre', value=word2idx_s[\"*\"])\n",
    "#TEST\n",
    "# print(\"Largo max inp test: \",max_inp_length_test)\n",
    "X_test = sequence.pad_sequences(dataX_test, maxlen=max_inp_len, padding='pre', value=word2idx_s[\"*\"])\n",
    "\n",
    "\"\"\" OUTPUT DATA (Destination language) \"\"\"\n",
    "#TRAIN\n",
    "max_out_length = max(map(len,dataY_train)) \n",
    "max_out_length_val = max(map(len,dataY_valid))\n",
    "max_out_length_test = max(map(len,dataY_test))\n",
    "max_out_len = max(max_out_length,max_out_length_val,max_out_length_test)\n",
    "print(\"Largo max out: \",max_out_len)\n",
    "word2idx_t[\"*\"] = 0 #padding symbol\n",
    "idx2word_t[0] = \"*\"\n",
    "n_words_t += 1  \n",
    "Y_train = sequence.pad_sequences(dataY_train, maxlen=max_out_len, padding='post', value=word2idx_t[\"*\"])\n",
    "#VALID\n",
    "# print(\"Largo max out val: \",max_out_length_val)\n",
    "Y_val = sequence.pad_sequences(dataY_valid, maxlen=max_out_len, padding='post', value=word2idx_t[\"*\"])\n",
    "#TEST\n",
    "# print(\"Largo max out test: \",max_out_length_test)\n",
    "Y_test = sequence.pad_sequences(dataY_test, maxlen=max_out_len, padding='post', value=word2idx_t[\"*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Para evitar que la red obtenga una ganancia por imitar/predecir el símbolo de padding que está bastante presente en los datos coloque un peso sobre éste clase, con valor 0, así se evita que tenga impacto en la función objetivo. Ya que keras no soporta directamente ésto en series de tiempo coloque el peso a cada instante de tiempo de cada dato de entrenamiento dependiendo de su clase. Comente sobre alguna otra forma en que se podría manejar el evitar que la red prediga en mayoría el símbolo de padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_weights = np.ones(n_words_t)\n",
    "c_weights[0] = 0 #padding class masked\n",
    "sample_weight = np.zeros(Y_train.shape)\n",
    "for i in range(sample_weight.shape[0]):\n",
    "    sample_weight[i] = c_weights[Y_train[i,:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede lograr el mismo efecto utilizando el parametro *mask_zero = True* en la capa de embedding de keras, la documentación de keras dice: <br>\n",
    ">If mask_zero is set to True, the input value 0 will be a special \"padding\" that should be masked out.\n",
    "Index 0 cannot be used in the vocabulary. (_Embedding Layer_ doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Para lograr la tarea defina una red recurrente del tipo encoder-decoder como la que se presenta en la siguiente imágen.\n",
    "En primer lugar defina el Encoder que procesara el texto de entrada y retornará un solo vector final, haciendo uso de las capas ya conocidas de Embedding para generar un vector denso de palabra y GRU, pero en su versión acelerada para GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Paralizis\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#ENCODER\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,CuDNNGRU\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_len))\n",
    "model.add(CuDNNGRU(64, return_sequences=True))\n",
    "model.add(CuDNNGRU(128, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "model.add(RepeatVector(max_out_length)) #conection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 45, 100)           610300    \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (None, 45, 64)            31872     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 51, 128)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_3 (CuDNNGRU)       (None, 51, 128)           99072     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_4 (CuDNNGRU)       (None, 51, 64)            37248     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 51, 9236)          600340    \n",
      "=================================================================\n",
      "Total params: 1,453,328\n",
      "Trainable params: 1,453,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DECODER\n",
    "from keras.layers import CuDNNGRU, TimeDistributed,Dense\n",
    "model.add(CuDNNGRU(128, return_sequences=True))\n",
    "model.add(CuDNNGRU(64, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85107, 51, 1)\n",
      "(10000, 51, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_train_exp = np.reshape(Y_train, (Y_train.shape[0], Y_train.shape[1],1))\n",
    "Y_val_exp = np.reshape(Y_val, (Y_val.shape[0], Y_val.shape[1], 1))\n",
    "print(Y_train_exp.shape)\n",
    "print(Y_val_exp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Paralizis\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 85107 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "85107/85107 [==============================] - 121s 1ms/step - loss: 6.2936 - val_loss: 12.5617\n",
      "Epoch 2/3\n",
      "85107/85107 [==============================] - 117s 1ms/step - loss: 6.0092 - val_loss: 13.5133\n",
      "Epoch 3/3\n",
      "85107/85107 [==============================] - 110s 1ms/step - loss: 5.9344 - val_loss: 14.0965\n",
      "Wall time: 5min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b091c34f28>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 256\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')\n",
    "model.fit(X_train, Y_train_exp, epochs=3, batch_size=BATCH_SIZE,validation_data=(X_val, Y_val_exp),\n",
    "         sample_weight = sample_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Debido a lo costoso de tener una red completamente recurrente para entrenar y poder experimentar, cambie el modelo que procesa el Encoder por una red convolucional, reduciendo el número de capas pero aumentando las neuronas. Utilice tamaños de kernel igual a 5 y funciones de activaciones relu. Se agregan capas de BatchNormalization debido a que en el Decoder contamos con redes recurrentes que tienen capa activación distinta a la usada por las convoluciones. La capa de GlobalMaxPooling1d es lo que permite reducir toda la información extraída a un único vector, como se realizó anteriormente con return_sequences=False, comente sobre la ganancia o desventaja de ésto vs la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 45, 100)           610300    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 45, 256)           128256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 45, 256)           1024      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 45, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 45, 256)           1024      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 51, 256)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_5 (CuDNNGRU)       (None, 51, 256)           394752    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 51, 9236)          2373652   \n",
      "=================================================================\n",
      "Total params: 3,836,944\n",
      "Trainable params: 3,835,920\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D,MaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D,BatchNormalization\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_len))\n",
    "model.add(Conv1D(256, 5, padding='same', activation='relu', strides=1))\n",
    "model.add(BatchNormalization()) #for stability\n",
    "model.add(Conv1D(256, 5, padding='same', activation='relu', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D()) #aka to return_sequences=False\n",
    "model.add(RepeatVector(max_out_length)) #conection\n",
    "model.add(CuDNNGRU(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Paralizis\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 85107 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "85107/85107 [==============================] - 162s 2ms/step - loss: 5.6565 - val_loss: 14.3453\n",
      "Epoch 2/20\n",
      "85107/85107 [==============================] - 149s 2ms/step - loss: 4.6114 - val_loss: 14.5112\n",
      "Epoch 3/20\n",
      "85107/85107 [==============================] - 144s 2ms/step - loss: 4.0088 - val_loss: 14.4619\n",
      "Epoch 4/20\n",
      "85107/85107 [==============================] - 144s 2ms/step - loss: 3.5915 - val_loss: 14.4253\n",
      "Epoch 5/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 3.2682 - val_loss: 14.3974\n",
      "Epoch 6/20\n",
      "85107/85107 [==============================] - 144s 2ms/step - loss: 3.0092 - val_loss: 14.3700\n",
      "Epoch 7/20\n",
      "85107/85107 [==============================] - 147s 2ms/step - loss: 2.8132 - val_loss: 14.3655\n",
      "Epoch 8/20\n",
      "85107/85107 [==============================] - 144s 2ms/step - loss: 2.6352 - val_loss: 14.3485\n",
      "Epoch 9/20\n",
      "85107/85107 [==============================] - 144s 2ms/step - loss: 2.4964 - val_loss: 14.3409\n",
      "Epoch 10/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 2.3659 - val_loss: 14.3395\n",
      "Epoch 11/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 2.2586 - val_loss: 14.3390\n",
      "Epoch 12/20\n",
      "85107/85107 [==============================] - 147s 2ms/step - loss: 2.1611 - val_loss: 14.3450\n",
      "Epoch 13/20\n",
      "85107/85107 [==============================] - 146s 2ms/step - loss: 2.0760 - val_loss: 14.3305\n",
      "Epoch 14/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 1.9948 - val_loss: 14.3329\n",
      "Epoch 15/20\n",
      "85107/85107 [==============================] - 146s 2ms/step - loss: 1.9274 - val_loss: 14.3313\n",
      "Epoch 16/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 1.8624 - val_loss: 14.3346\n",
      "Epoch 17/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 1.8054 - val_loss: 14.3332\n",
      "Epoch 18/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 1.7493 - val_loss: 14.3384\n",
      "Epoch 19/20\n",
      "85107/85107 [==============================] - 145s 2ms/step - loss: 1.6978 - val_loss: 14.3369\n",
      "Epoch 20/20\n",
      "85107/85107 [==============================] - 144s 2ms/step - loss: 1.6515 - val_loss: 14.3392\n",
      "Wall time: 48min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b19c8b5d68>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')\n",
    "model.fit(X_train, Y_train_exp, epochs=20, batch_size=256,validation_data=(X_val, Y_val_exp),\n",
    "         sample_weight = sample_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Visualice lo aprendido por el modelo sobre algunos datos del conjunto de entrenamiento y validación, comente lo observado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(y_indexs, data=\"target\"):\n",
    "    \"\"\" Predict until '-#end-' is seen \"\"\"\n",
    "    return_val = []\n",
    "    for indx_word in y_indexs:\n",
    "        if indx_word != 0: #start to predict\n",
    "            return_val.append(np.squeeze(indx_word))\n",
    "            if data == \"target\": #if target is predicting\n",
    "                if indx_word == word2idx_t[\"#end\"]:\n",
    "                    return return_val                \n",
    "    return return_val\n",
    "n_s = 100\n",
    "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n",
    "Y_set_pred = model.predict_classes(X_set[idx] )\n",
    "for i, n_sampled in enumerate(idx):\n",
    "    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n",
    "    print(\"Texto source: \", ' '.join(text_input))\n",
    "    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n",
    "    print(\"Texto target real: \", ' '.join( text_real))\n",
    "    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n",
    "    print(\"Texto target predicho: \", ' '.join(text_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] Dwarampudi, M., & Reddy, N. V. (2019). Effects of padding on LSTMs and CNNs. arXiv preprint [arXiv:1903.07288](https://arxiv.org/abs/1903.07288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
