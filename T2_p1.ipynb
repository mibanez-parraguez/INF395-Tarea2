{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T2_p1.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/mibanez-parraguez/T2_ANN/blob/master/T2_p1.ipynb","timestamp":1561446924859}],"collapsed_sections":["rW_xBzzGNLYe","VKnGdrP1NLYt","IDRuNb5yNLY9","fK_rC-_-NLZs","A5pfYJg6NLZ4","-c3YVUarlbN8","gZ5pq6Y2le6E","yEg7TTLYlv6c","qqmXKjXTlx8F","A3cBNHnLw2Bp","br_FG_S2xzzZ","48GjqEoRsTho","SEJXQqK6yCow"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0NmB-0cYNLW2","colab_type":"text"},"source":["<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n","\n","\n","<hr style=\"height:2px;border:none\"/>\n","<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n","\n","<H3 align='center'> Tarea 2 - Aplicaciones Recientes de Redes Neuronales </H3>\n","<H3 align='center'> Pregunta 1 - RNN sobre texto</H3>\n","<hr style=\"height:2px;border:none\"/>"]},{"cell_type":"markdown","metadata":{"id":"KCe8YMtYNLW-","colab_type":"text"},"source":["**Librerías**"]},{"cell_type":"code","metadata":{"id":"50-6jqzJNLXA","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dWUoBBcxNLXJ","colab_type":"text"},"source":["**Dataset**"]},{"cell_type":"code","metadata":{"id":"iaLCVMkUXA2R","colab_type":"code","colab":{}},"source":["drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxLzPdubNLXM","colab_type":"code","outputId":"d3678680-eb8c-4876-897e-69483921f9d9","executionInfo":{"status":"ok","timestamp":1563466389216,"user_tz":240,"elapsed":12224,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["fname = 'gdrive/My Drive/Colab Notebooks/ner.csv'\n","df_ner = pd.read_csv(fname, encoding =\"cp1252\", error_bad_lines=False)\n","df_ner.dropna(inplace=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"kDqzTeGbNLXY","colab_type":"text"},"source":["## a)\n","**Lema vs Palabra**\n","\n","Un lema es la base con significado de las palabras, por lo tanto no incluye inflexiones por persona, número, genero u otras. Por esto, al usar los lemas en vez de las palabras tal cual están en las frases, se obtiene un diccionario o vocabulario más reducido del dataset sin perder mucho significado.\n","\n","<br/>\n","\n","**Dimensiones dataset**\n","\n","El dataset se compone de 655.408 lemas en total, a partir de 14.287 lemas únicos. Existen 17 etiquetas únicas y excluyentes entre sí, en las que se clasifican los lemas.\n","\n","Considerando la organización en sentencias del dataset, se tiene que está compuesto por 30.000 frases de largo variable entre sí. El largo promedio es de 21,8 palabras o lemas, con un máximo de 70 lemas, aunque, sólo dos frases alcanzan este largo. "]},{"cell_type":"code","metadata":{"id":"FhR8FH_INLXb","colab_type":"code","outputId":"368122a7-9f69-4641-e07d-a8fb464f4e9a","executionInfo":{"status":"ok","timestamp":1563466389233,"user_tz":240,"elapsed":5030,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["dataset = df_ner.loc[:,[\"lemma\",\"word\",\"pos\",\"tag\", \"prev-iob\"]]\n","print(\"dataset.shape:\", dataset.shape)\n","dataset.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dataset.shape: (1050794, 5)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lemma</th>\n","      <th>word</th>\n","      <th>pos</th>\n","      <th>tag</th>\n","      <th>prev-iob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>thousand</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>__START1__</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>demonstr</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>have</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>march</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      lemma           word  pos tag    prev-iob\n","0  thousand      Thousands  NNS   O  __START1__\n","1        of             of   IN   O           O\n","2  demonstr  demonstrators  NNS   O           O\n","3      have           have  VBP   O           O\n","4     march        marched  VBN   O           O"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"JV-rQ-9-NLXq","colab_type":"code","colab":{}},"source":["n_used = 655409 # 30,000 sentences\n","dataX_raw,dataY_raw = [],[]\n","lemmas,labels = set(), set()  #uniques\n","sentence= []\n","labels_sentence = []\n","for fila in dataset.values[:n_used]:\n","    if fila[-1]==\"__START1__\":\n","        dataX_raw.append(sentence)\n","        dataY_raw.append(labels_sentence)\n","        sentence= []\n","        labels_sentence = []\n","    lemmas.add(fila[0])\n","    labels.add(fila[3])\n","    sentence.append(fila[0]) #add lemma\n","    labels_sentence.append(fila[3]) #TAG    \n","dataX_raw = dataX_raw[1:]\n","dataY_raw = dataY_raw[1:]\n","\n","sentences_len = pd.Series([len(x) for x in dataX_raw])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoWJn5fJtcx5","colab_type":"code","outputId":"a1edb399-db69-4ed5-e64a-d718fd210bf1","executionInfo":{"status":"ok","timestamp":1563420302262,"user_tz":240,"elapsed":1017,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["print(\"Número total de lemmas: {}\".format(sentences_len.sum()))\n","print(\"         #Lemas únicos: {}\".format(len(lemmas)))\n","print(\"     #Etiquetas únicas: {}\".format(len(labels)))\n","print()\n","print(\"Número de frases: {}\".format(len(sentences_len)))\n","print(\"  Largo promedio: {:.1f}\".format(sentences_len.mean()))\n","print(\"    Largo máximo: {}    (#frases: {})\".format(sentences_len.max(), sentences_len.value_counts()[70]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Número total de lemmas: 655408\n","         #Lemas únicos: 14287\n","     #Etiquetas únicas: 17\n","\n","Número de frases: 30000\n","  Largo promedio: 21.8\n","    Largo máximo: 70    (#frases: 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EbRCT7xLNLX4","colab_type":"text"},"source":["## b)  Largo sentencias y frecuencia lemas\n"]},{"cell_type":"markdown","metadata":{"id":"Uj0nN0Ll41ol","colab_type":"text"},"source":["### Distribución del largo de sentencias\n","\n","En el histograma más abjo se aprecia que la mayoría de las sentencias del dataset se componen de entre 10 a 30 palabras, promediando un largo de alrededor de 20. También se observa una cola en la distribución con frases de largo mayor, alcanzando un máximo de 70 palabras. Esto implica que al hacer _padding_, una cantidad importante de frases van a contener más _tokens_ de _padding_ que palabras originales de la frase. "]},{"cell_type":"code","metadata":{"id":"xiLsnDTNNLYB","colab_type":"code","outputId":"138c5d08-bef6-46c5-ebec-f9739fd3f8cd","executionInfo":{"status":"ok","timestamp":1563421648258,"user_tz":240,"elapsed":1230,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":404}},"source":["# Distro longitud sentencias\n","sent_len_hist = sentences_len.value_counts().sort_index()\n","bins = sent_len_hist.index\n","\n","fig, ax = plt.subplots(figsize=(12, 6))\n","ax.bar(bins, sent_len_hist.values)\n","ax.set_xlabel('len(frase)')\n","ax.set_ylabel('# frases')\n","ax.set_title(\"Histograma: Largo sentencias\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAt0AAAGDCAYAAAD3W6zoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"MAqoz14A4_B9","colab_type":"text"},"source":["### Frecuencia lemas & zipf's law\n","\n","A continuación se grafica la distribución de probabilidad de la frecuencia de cada lema en el dataset completo. Se puede apreciar que la frecuencia de cada lema es aproximadamente al inverso del ranking según frecuencia de dicho lema, aunque con un _shift_ vertical. Por lo que esta distribución sigue una ley de Zipf.\n","\n","En el gráfico se agregaron tres distribuciones de zipf con parámetros $a_1=0.8$, $a_2 = 1$ y $a_3 = 1.2$ para comparar con la distribución del dataset."]},{"cell_type":"code","metadata":{"id":"mMkadYjHNLX6","colab_type":"code","outputId":"99dc9364-74f0-4714-f116-458c39051483","executionInfo":{"status":"ok","timestamp":1562910525877,"user_tz":240,"elapsed":2367,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["# zipf's law\n","zipf_i = lambda a, i : 1/np.power(i,a)\n","vzipf = np.vectorize(zipf_i)\n","\n","# Frecuencia palabras (lemma)\n","lemma_pmf = dataset[\"lemma\"].value_counts(normalize=True)\n","\n","# PLOT\n","bins = range(1, lemma_pmf.count()+1)\n","fig, ax = plt.subplots(figsize=(12, 6))\n","ax.loglog(bins, lemma_pmf.values)\n","ax.loglog(bins, vzipf(1, bins), '--', lw=0.8, label=\"a={:1.1f}\".format(1))\n","ax.loglog(bins, vzipf(1.2, bins), '--', lw=0.8, label=\"a={:1.1f}\".format(1.2))\n","ax.loglog(bins, vzipf(.8, bins), '--', lw=0.8, label=\"a={:1.1f}\".format(0.8))\n","ax.set_xlabel('lema')\n","ax.set_ylabel('P')\n","ax.set_title(\"Distribución probabilidad frec. lema\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAt0AAAGHCAYAAABsyu7+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1DFLtyRJktSx/wAOoMRcxheTqgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ukKerQAVAdN6","colab_type":"text"},"source":["### Frecuencia tags\n","\n","Se observa que cerca del $84\\%$ de ocurrencias son del tag 'O' que pertenece a las palabras o lemas que no describen una entidad, que, como es de esperar,  corresponde a la gran mayoría de las palabras en el dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"72b09f53-adb3-43c8-ef2c-b15535e23f0a","executionInfo":{"status":"ok","timestamp":1562910777207,"user_tz":240,"elapsed":1656,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"id":"hdV_HetFsdAh","colab":{"base_uri":"https://localhost:8080/","height":404}},"source":["# Frecuencia tags\n","tag_pmf = dataset[\"tag\"].value_counts(normalize=True)\n","\n","# PLOT\n","fig, ax = plt.subplots(figsize=(12, 6))\n","ax.plot(tag_pmf.index, tag_pmf.values)\n","#ax.set_yscale('log')\n","ax.set_xlabel('tag')\n","ax.set_ylabel('P')\n","ax.set_title(\"Distribución probabilidad frec. tag\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAtQAAAGDCAYAAAALTociAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"-AUgpwfTvTw5","colab_type":"code","outputId":"eea41694-6438-429a-d80a-3a1395513efb","executionInfo":{"status":"ok","timestamp":1562910737920,"user_tz":240,"elapsed":715,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":330}},"source":["tag_pmf"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["O        0.846953\n","B-geo    0.035711\n","B-tim    0.019217\n","B-org    0.019208\n","I-per    0.016542\n","B-per    0.016189\n","I-org    0.015738\n","B-gpe    0.015600\n","I-geo    0.007051\n","I-tim    0.005994\n","B-art    0.000413\n","B-eve    0.000331\n","I-eve    0.000283\n","I-art    0.000266\n","I-gpe    0.000218\n","B-nat    0.000215\n","I-nat    0.000072\n","Name: tag, dtype: float64"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"markdown","metadata":{"id":"rW_xBzzGNLYe","colab_type":"text"},"source":["## c) Codificación de _tags_ y _lemmas_\n","\n","Se codifica cada lema y tag con un índice para poder modelar el ejercicio. Hay 14.287 lemas y 17 tags únicos en la fracción del dataset a usar. En la siguiente sección, se agrega un token más para realizar el _padding_ y dejar las frases del mismo largo.\n","\n","Se tiene, además, que la longitud máxima encontradas entre las sentencias es de 70 lemas."]},{"cell_type":"code","metadata":{"id":"O1pjHC97NLYo","colab_type":"code","colab":{}},"source":["n_labels = len(labels)\n","lab2idx = {t: i for i, t in enumerate(labels)}\n","dataY = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY_raw] #Converting tags to indexs\n","n_lemmas = len(lemmas)\n","lemma2idx = {w: i for i, w in enumerate(lemmas)} \n","dataX = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX_raw] #Converting text to indexs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVPuDz7LNLYh","colab_type":"code","outputId":"c4ed7455-06a5-4b29-a9f6-9d2901795824","executionInfo":{"status":"ok","timestamp":1563421754903,"user_tz":240,"elapsed":496,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["print(\" total tags distintos: {}\".format(n_labels))\n","print(\"total lemas distintos: {}\".format(n_lemmas))\n","print(\"Longitud máxima de sentencias: {}\".format(max(sentences_len)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" total tags distintos: 17\n","total lemas distintos: 14287\n","Longitud máxima de sentencias: 70\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VKnGdrP1NLYt","colab_type":"text"},"source":["## d) pre-_padding_\n","\n","En el paper Dwarampudi & Reddy (2019). \"Effects of padding on LSTMs and CNNs\" [[1]](#refs), los autores, encuentran que en redes LSTM, el _accuracy_ del modelo es mejor usando _pre-padding_ en las secuencias. Realizaron las pruebas con un problema de clasificación de _tweets_ (mensajes positivos o negativos) preparando los ejemplos con _pre-padding_ y _post-padding_ y probaron el _accuracy_ de modelos usando redes LSTM y CNN. Según los resultados que obtuvieron, para redes LSTM el _accuracy_ con _pre-padding_ fue de $80.3%$ y con _post-padding_ fue de $50.1%$ con los conjuntos de ejemplo de test. Para redes CNN no encontraron diferencia y el _accuracy_ logrado fue de alrededor de $74$. Por este motivo, se decide usar _pre-padding_.\n","\n","Por otro lado, como ya se analizó en la sección b), la mayoría de las frases tienen longitud de entre 10 y 30 lemas. Sin embargo, la longitud máxima encontrada es de 70 lemas. Es decir, una gran cantidad de frases van a tener la mitad o más de su longitud rellenados con el token de _padding_. Esto implica que, con el _padding_ se va a agregar mucho costo al procesamiento del modelo."]},{"cell_type":"code","metadata":{"id":"QLRWFt0K9G9d","colab_type":"code","outputId":"ad64991d-97a1-403e-a480-12f53b70bff1","executionInfo":{"status":"ok","timestamp":1563466397211,"user_tz":240,"elapsed":1866,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.preprocessing import sequence"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"z59_rl9cNLYw","colab_type":"code","colab":{}},"source":["lemma2idx[\"_PRE-PADDING_\"] = n_lemmas #add fullfill lemma and tag to the dictionary\n","lab2idx[\"_PRE-PADDING_\"] = n_labels\n","n_labels +=1\n","n_lemmas +=1\n","\n","X = sequence.pad_sequences(dataX, maxlen=max(sentences_len), padding='pre', value=lemma2idx[\"_PRE-PADDING_\"])\n","y = sequence.pad_sequences(dataY, maxlen=max(sentences_len), padding='pre', value=lab2idx[\"_PRE-PADDING_\"])\n","del dataY[:],dataX[:]\n","del dataY, dataX"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t97bIEjANLY2","colab_type":"code","outputId":"684ee459-67be-4a80-a2db-b24841d29e70","executionInfo":{"status":"ok","timestamp":1563423233867,"user_tz":240,"elapsed":874,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(\"Código lemma pre-padding: {}\".format(lemma2idx[\"_PRE-PADDING_\"]))\n","print(\"   Tag lemma pre-padding: {}\".format(lab2idx[\"_PRE-PADDING_\"]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Código lemma pre-padding: 14287\n","   Tag lemma pre-padding: 17\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IDRuNb5yNLY9","colab_type":"text"},"source":["## e) Word2Vec\n","\n","En el siguiente código se usa el módulo `word2vec` de `gensim` para mapear las palabras del _corpus_ a un espacio vectorial. Específicamente, se utiliza el modelo _skip-gram_ que consiste en una capa escondida de $N_{\\text{embeddings}}$ neuronas lineales y una capa de salida _softmax_ con un número de neuronas igual a la cantidad de lemas únicos en el diccionario. El parámetro $N_{\\text{embeddings}}$ corresponde al número de _embeddings_ y la dimensión del espacio vectorial.\n","\n","La red se entrena usando pares de palabras, con la palabra _target_ (input de la red) y con la palabra _output_ que debe entregar la red. Para cada palabra (_target_) en cada documento de entrenamiento se toma un conjunto de $n$ palabras (_window size_) que antecede al _target_ y $n$ palabras que le suceden. Este conjunto forma el contexto del _target_ y los pares de palabras de entrenamiento se obtienen formando pares con el _target_ y cada una de las palabras en su contexto.\n","\n","Una vez entrenada, la red debe dar como _output_ la probabilidad de cada una de las palabras del _corpus_ pertenezca al contexto de la palabra que se le entrega como _imput_.\n","\n","En esta red neuronal, lo que realmente importan son los pesos de las neuronas en la capa escondida. Representando esta capa como una matriz de dimensión $N_{\\text{lemmas}}\\times N_{\\text{embeddings}}$, cuyo contenido son los pesos de las neuronas, la fila i-ésima, de esta matriz, corresponde a la representación vectorial de la palabra i-ésima del _corpus_. Por como se entrena este modelo, si dos palabras tienen contexto similar, entonces, van a tener vectores muy cercanos o similares.\n","\n","En el código siguiente, se especifica además el parámetro `min_count = 3` que deja fuera del modelo todos los lemas con frecuencia menor a la especificada. En esta sección en concreto, se dejan afuera los lemas con frecuencia menor a 3, por lo que, el total de lemas que se usan para generar su representación vectorial es de 8.008 (56% del total).\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"nPQisJbRNLZB","colab_type":"code","colab":{}},"source":["from gensim.models import Word2Vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_U1fLRtNLZH","colab_type":"code","colab":{}},"source":["EMBEDDING_DIM = 32\n","window_size = 5\n","nb_epoch = 5\n","batch_size = 6000\n","min_count = 3\n","model = Word2Vec(dataX_raw, size=EMBEDDING_DIM, window=window_size, \n","                 batch_words=batch_size, iter=nb_epoch, min_count=min_count, \n","                 negative=5, sg=1) #sg=1 mean skip-gram\n","\n","embeddings_index = {vocab_word: model.wv[vocab_word] for vocab_word in model.wv.vocab}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxaNzkiaNLZT","colab_type":"code","outputId":"bd2c0290-6bd1-4112-d84e-d8b6633eae8a","executionInfo":{"status":"ok","timestamp":1563422188198,"user_tz":240,"elapsed":7340,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["aux1 = dataset[:n_used][\"lemma\"].value_counts()\n","aux_c = aux1[aux1<min_count].size\n","print(\"Número total de lemmas:\", n_lemmas-1) # menos token padding\n","print(\"Lemas con frecuencia >= {}: {}\".format(min_count, n_lemmas - 1 - aux_c))\n","print(\"lemas mantenidos por Word2Vec: {}\".format(len(embeddings_index.keys())))\n","del aux1, aux_c"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Número total de lemmas: 14287\n","Lemas con frecuencia >= 3: 8008\n","lemas mantenidos por Word2Vec: 8008\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t4K3yGU4yb4F","colab_type":"text"},"source":["A continuación, se crea la matriz de _embeddings_, para todo el _corpus_, agregando vectores $\\vec{0}$ a la matriz generada por `Word2vec` por cada lema que fue descartado en el paso anterior (por tener frecuencia menor a `min_count`)."]},{"cell_type":"code","metadata":{"id":"D4fBoVzAyahN","colab_type":"code","colab":{}},"source":["embedding_matrix = np.zeros((n_lemmas, EMBEDDING_DIM))\n","for word, i in lemma2idx.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DrrXYTzly5Df","colab_type":"text"},"source":["Finalmente, se crean los  _one-hot vectors_ para las secuencias de salidas de entrenamiento. Esto quiere decir que los tags en las secuencias de salida serán transformados en vectores de dimensión `n_labels` con valor $1$ en la posición correspondiente al _id_ del tag y $0$ en las otras posiciones.\n"]},{"cell_type":"code","metadata":{"id":"PqrYOHm5yqSF","colab_type":"code","outputId":"8bfd155a-62ec-4687-eb0f-3654edcd9270","executionInfo":{"status":"ok","timestamp":1563466413979,"user_tz":240,"elapsed":753,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.utils import to_categorical\n","y_cat = np.asarray([to_categorical(i, num_classes=n_labels) for i in y])\n","y_cat.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30000, 70, 18)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"BYAYPAOONLZf","colab_type":"text"},"source":["## f) Conjuntos de entrenamiento y de prueba\n","\n","El dataset, de 30.000 frases o secuencias, es dividido con el 60% para el conjunto de entrenamiento, 20% para el conjunto de validación y  20% para el conjunto de prueba. De esta forma el _input_ de entrenamiento queda con 18.000 secuencias, todas de largo 70 (por el _pre-padding_ hecho previamente), y el _output_ del entrenamiento corresponden a 18.000 matrices de dimensiones 70 x 18 (largo de secuencia x número de tags o clases). Similar a lo anterior para los conjuntos de vaidación y de prueba, pero con 6.000 secuencias cada uno.\n"]},{"cell_type":"code","metadata":{"id":"WgQHeYd-zAoM","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_aux, X_test, y_aux, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=22)\n","X_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, test_size=0.25, random_state=22)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_cyRsFU5PaN","colab_type":"code","outputId":"ee2abcf5-9cb1-4bb5-e0a1-06d5caae3cc2","executionInfo":{"status":"ok","timestamp":1563423911827,"user_tz":240,"elapsed":737,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["print(\"Dimensiones conjunto entrenamiento:\\n   X = {}\\n   Y = {}\\n\".format(X_train.shape, y_train.shape))\n","print(\"Dimensiones conjunto validación:\\n   X = {}\\n   Y = {}\\n\".format(X_val.shape, y_val.shape))\n","print(\"Dimensiones conjunto prueba:\\n   X = {}\\n   Y = {}\".format(X_test.shape, y_test.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dimensiones conjunto entrenamiento:\n","   X = (18000, 70)\n","   Y = (18000, 70, 18)\n","\n","Dimensiones conjunto validación:\n","   X = (6000, 70)\n","   Y = (6000, 70, 18)\n","\n","Dimensiones conjunto prueba:\n","   X = (6000, 70)\n","   Y = (6000, 70, 18)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fK_rC-_-NLZs","colab_type":"text"},"source":["## g) Red LSTM con pesos pre-calculados por `Word2Vec`."]},{"cell_type":"markdown","metadata":{"id":"a1j0JqKmZ1CH","colab_type":"text"},"source":["### Entrenamiento modelo\n","\n","El siguiente modelo se compone de una capa de _embeddings_, una capa recurrente LSTM y una capa densa _softmax_ de 18 neuronas que se aplica a cada paso de las secuencias por medio del _wraper_ `keras.layers.TimeDistributed(layer)`.\n","\n","La capa de _embedding_ hace el mapeo de cada lema en la secuencia de entrada al espacio vectorial de 32 dimensiones. Los pesos (o _word vectors_) son los valores ya calculados con `Word2Vec`. La dimensión de _embeddings_ es de 32 y el _corpus_ contiene 14.287 lemas únicos, luego, se tienen 457.216 pesos en esta capa, los que no serán entrenados más en esta fase.\n","\n","Luego, viene la capa recurrente de compuertas LSTM. Se define con 100 unidades. Su _output_ tendrá dimensión `batch x 70 x 100`. En esta capa se tienen 53.200 parámetros entrenables.\n","\n","Finalmente, se tiene la capa _output_ como una capa densa _softmax_ que se aplica en cada paso de la secuencia, haciendo que esta red sea _many-to-many_. La capa tiene 18 neuronas, que es el número de etiquetas únicas, y por cada lema en la secuencia de entrada se devuelve la probabilidad de que pertenezca a una de las 18 etiquetas. En esta capa se tienen 1.818 parámetros entrenables que corresponden a los pesos y biases de las 18 neuronas para los 100 datos de entrada que le pasa la capa anterior.\n","\n","El modelo tiene en total 55.018 parámetros entrenables y demora alrededor de cuatro minutos en ser entrenado usando google colab con GPU activado."]},{"cell_type":"code","metadata":{"id":"dbP0z-hmMyE8","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, Dense, Dropout,TimeDistributed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_05gBe5NLZt","colab_type":"code","outputId":"3cb38906-c49e-4bde-f160-1ef3c0074387","executionInfo":{"status":"ok","timestamp":1563426577478,"user_tz":240,"elapsed":1114,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":278}},"source":["max_input_lenght=max(sentences_len) # Largo de secuencias: 70 lemas fijo.\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_input_lenght,\n","                    trainable=False, weights = [embedding_matrix]))\n","model.add(LSTM(units=100,return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_labels, activation='softmax')))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 70, 32)            457216    \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 70, 100)           53200     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 70, 100)           0         \n","_________________________________________________________________\n","time_distributed_2 (TimeDist (None, 70, 18)            1818      \n","=================================================================\n","Total params: 512,234\n","Trainable params: 55,018\n","Non-trainable params: 457,216\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YPBynbJhKNyQ","colab_type":"code","outputId":"1a8764ea-3f16-4feb-99ba-cff643eba57c","executionInfo":{"status":"ok","timestamp":1563426838470,"user_tz":240,"elapsed":245316,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":573}},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 18000 samples, validate on 6000 samples\n","Epoch 1/15\n","18000/18000 [==============================] - 17s 967us/step - loss: 0.3808 - val_loss: 0.2595\n","Epoch 2/15\n","18000/18000 [==============================] - 16s 912us/step - loss: 0.2190 - val_loss: 0.1757\n","Epoch 3/15\n","18000/18000 [==============================] - 16s 902us/step - loss: 0.1595 - val_loss: 0.1381\n","Epoch 4/15\n","18000/18000 [==============================] - 16s 907us/step - loss: 0.1341 - val_loss: 0.1208\n","Epoch 5/15\n","18000/18000 [==============================] - 16s 897us/step - loss: 0.1205 - val_loss: 0.1111\n","Epoch 6/15\n","18000/18000 [==============================] - 16s 916us/step - loss: 0.1116 - val_loss: 0.1030\n","Epoch 7/15\n","18000/18000 [==============================] - 16s 890us/step - loss: 0.1049 - val_loss: 0.0980\n","Epoch 8/15\n","18000/18000 [==============================] - 16s 885us/step - loss: 0.0998 - val_loss: 0.0929\n","Epoch 9/15\n","18000/18000 [==============================] - 16s 893us/step - loss: 0.0956 - val_loss: 0.0900\n","Epoch 10/15\n","18000/18000 [==============================] - 16s 894us/step - loss: 0.0921 - val_loss: 0.0863\n","Epoch 11/15\n","18000/18000 [==============================] - 16s 883us/step - loss: 0.0893 - val_loss: 0.0841\n","Epoch 12/15\n","18000/18000 [==============================] - 16s 895us/step - loss: 0.0869 - val_loss: 0.0821\n","Epoch 13/15\n","18000/18000 [==============================] - 16s 887us/step - loss: 0.0851 - val_loss: 0.0801\n","Epoch 14/15\n","18000/18000 [==============================] - 16s 897us/step - loss: 0.0832 - val_loss: 0.0790\n","Epoch 15/15\n","18000/18000 [==============================] - 16s 891us/step - loss: 0.0817 - val_loss: 0.0781\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f815254c320>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"sydHjwxUXOnw","colab_type":"text"},"source":["### Evaluación modelo\n","\n","Se utiliza el modelo para predecir los tag sobre el conjunto de testing elaborado anterioremente. Antes de evaluar el resultado, se elimina el _pre-tagging_ en los arreglos con las predicciones, `y_pred`, y con los valores reales, `y_true`, para evitar que se evalue el _tagging_ como parte de la predicción del modelo. \n","\n","Luego, se procede a comparar el número de tags presentes en los valores reales con el número de tags predichos por el modelo. Se observa que existen 17 tags únicos en el set, pero el modelo sólo predice 10 de estos tags. Además, en varias secuencias predichas, el modelo devolvió más _pre-padding_ que el real por lo que este tag especial aparece en el set de tags predichos pero no está presente en el set de tags reales.\n","\n","Lo anterior genera un problema al calcular el _f1-score_ del modelo, pues hay tags que no tienen ejemplos predichos, así como, hay un tag predicho (el _pre-tagging_) que no existe entre los valores reales. La función `f1_score` de la librería `sklearn` resuelve con valor $0.0$ los casos estos casos conflictivos para poder calcular el puntaje.\n","\n","Por último, se debe recordar que se tiene un importante desbalance en la cantidad de ejemplos para cada tag. Como se vió en la sección b), el tag 'O' representa más del 80% de los ejemplos. Este tag corresponde a las palabras o tokens que no identifican a alguna identidad (persona, lugar, tiempo, entidad geográfica, entidad geopolitica, etc). Para evitar que la sobrerepresentación de este tag influya tanto en el resultado del _f1-score_ se utiliza _macro-average_ como parte del calculo. Con este método, se calcula el puntaje para cada tag por separado y se promedian los resultados, con esto, cada tag aporta con el mismo peso al puntaje final. Si se usara el método _micro-average_, al calcular sobre todos los ejemplos directamente, cada ejemplo aporta individualmente al puntaje sin ser separado por clase o tag, resultando en que el tag 'O' tenga mucho más peso en el _score_ que el resto de los tags que son más importantes.\n","\n","Entonces, con el parámetro `average='macro'`, se tiene que el puntaje obtenido por el modelo es de $0.727$"]},{"cell_type":"code","metadata":{"id":"5B85txzqX_L7","colab_type":"code","colab":{}},"source":["from sklearn.metrics import f1_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"biPctJYZaZqR","colab_type":"code","outputId":"9c6a9b8e-c735-4fcf-f050-a583d2d508d1","executionInfo":{"status":"ok","timestamp":1563427055360,"user_tz":240,"elapsed":11142,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["# Construccion y_true\n","dataY = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY_raw]\n","_, y_true = train_test_split(dataY, test_size=0.2, random_state=22)\n","del dataY[:], dataY\n","\n","# Prediccion modelo\n","dataY_pred = model.predict_classes(X_test,verbose=0)\n","y_pred = [dataY_pred[i][-len(y_true[i]):] for i in range(len(dataY_pred))]\n","\n","# labels unicos en cada set.\n","uniq_true = set()\n","uniq_pred = set()\n","for i in range(len(y_pred)):\n","  if len(y_pred[i]) != len(y_true[i]):\n","    print(\"ERROR {}, {} - {}\".format(i, len(dataY_pred[i]), len(dataY[i])))\n","  for j in range(len(y_true[i])):\n","    uniq_true.add(y_true[i][j])\n","    uniq_pred.add(y_pred[i][j])\n","\n","print(\"labels in y_true: {}\".format(uniq_true))\n","print(\"labels in y_pred: {}\".format(uniq_pred))\n","print(\"----\\n\")\n","\n","#F1 score\n","f1_score_bydata = [f1_score(true, pred ,average='macro') for true, pred in zip(y_true, y_pred) ]\n","print(\"\\n\\nF1 score on test: \", np.mean(f1_score_bydata) )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["labels in y_true: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n","labels in y_pred: {0, 2, 3, 4, 8, 9, 10, 11, 12, 13, 17}\n","----\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n","  'recall', 'true', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","F1 score on test:  0.726802154786343\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A5pfYJg6NLZ4","colab_type":"text"},"source":["## h) Red LSTM entrenando _embeddings_"]},{"cell_type":"markdown","metadata":{"id":"-c3YVUarlbN8","colab_type":"text"},"source":["### Entrenamiento modelo\n","\n","En este modelo se agrega la capa de _embeddings_ al entrenamiento. El punto de partida de esta capa (inicialización de pesos) son los pesos encontrados con `Word2Vec`. En este nuevo modelo, se tienen 512.234 parámetros en total. De estos, 457.216 (89,3%) parámetros corresponden a la capa de _embeddings_,\n","\n","La ejecución del entrenamiento tarda entre un poco más de cuatro minutos a cuatro minutos y medios en google colab con GPU activado. Lo que da, un aumento de entre 10s a 30s más de ejecución con respecto al modelo anterior."]},{"cell_type":"code","metadata":{"id":"8Ol1sLhsNLZ6","colab_type":"code","outputId":"fa3c04e1-0c24-4880-ea8f-ff49ab1314ad","executionInfo":{"status":"ok","timestamp":1563427178598,"user_tz":240,"elapsed":1271,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":278}},"source":["model = Sequential()\n","model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_input_lenght,\n","                    trainable=True, weights = [embedding_matrix]))\n","model.add(LSTM(units=100,return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_labels, activation='softmax')))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 70, 32)            457216    \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 70, 100)           53200     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 70, 100)           0         \n","_________________________________________________________________\n","time_distributed_3 (TimeDist (None, 70, 18)            1818      \n","=================================================================\n","Total params: 512,234\n","Trainable params: 512,234\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kqh9dYiIlXLs","colab_type":"code","outputId":"c482285c-f6f4-494f-88a8-13f5f38b9837","executionInfo":{"status":"ok","timestamp":1563427539753,"user_tz":240,"elapsed":256330,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":573}},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 18000 samples, validate on 6000 samples\n","Epoch 1/15\n","18000/18000 [==============================] - 18s 1ms/step - loss: 0.7150 - val_loss: 0.2790\n","Epoch 2/15\n","18000/18000 [==============================] - 17s 971us/step - loss: 0.2259 - val_loss: 0.1660\n","Epoch 3/15\n","18000/18000 [==============================] - 17s 943us/step - loss: 0.1374 - val_loss: 0.1077\n","Epoch 4/15\n","18000/18000 [==============================] - 17s 950us/step - loss: 0.0943 - val_loss: 0.0790\n","Epoch 5/15\n","18000/18000 [==============================] - 17s 944us/step - loss: 0.0730 - val_loss: 0.0664\n","Epoch 6/15\n","18000/18000 [==============================] - 17s 938us/step - loss: 0.0623 - val_loss: 0.0595\n","Epoch 7/15\n","18000/18000 [==============================] - 17s 941us/step - loss: 0.0557 - val_loss: 0.0558\n","Epoch 8/15\n","18000/18000 [==============================] - 17s 942us/step - loss: 0.0512 - val_loss: 0.0522\n","Epoch 9/15\n","18000/18000 [==============================] - 17s 948us/step - loss: 0.0475 - val_loss: 0.0500\n","Epoch 10/15\n","18000/18000 [==============================] - 17s 933us/step - loss: 0.0448 - val_loss: 0.0484\n","Epoch 11/15\n","18000/18000 [==============================] - 16s 911us/step - loss: 0.0425 - val_loss: 0.0471\n","Epoch 12/15\n","18000/18000 [==============================] - 17s 925us/step - loss: 0.0403 - val_loss: 0.0462\n","Epoch 13/15\n","18000/18000 [==============================] - 16s 914us/step - loss: 0.0385 - val_loss: 0.0452\n","Epoch 14/15\n","18000/18000 [==============================] - 17s 926us/step - loss: 0.0371 - val_loss: 0.0445\n","Epoch 15/15\n","18000/18000 [==============================] - 17s 923us/step - loss: 0.0355 - val_loss: 0.0442\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f813baf4588>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"gZ5pq6Y2le6E","colab_type":"text"},"source":["### Evaluación modelo\n","\n","Este modelo obtiene como puntaje: $0.834$. Cerca de $0.1$ puntos más que el modelo anterior. Esto puede deberse a que al seguir entrenando la capa de _embeddings_ se obtiene una representación vectorial mejor o más útil para la red. Hay que tener presente que al usar el _autoenconder_ `Word2Vec`, se excluyeron 6.279 lemas (44%) por tener frecuencia menor a 3 en el corpus y al reconstruir la matriz de _embeddings_, estos lemas, fueron representados con vectores $\\vec{0}$.\n","\n","Es importante notar que este modelo predice aún menos tags que el anterior. Sólo 11 tags son predichos y se sigue usando el tag de _pre-padding_ cuando no corresponde, por esto aparece en el conjunto de tags predichos y no en el de valores reales. "]},{"cell_type":"code","metadata":{"id":"mMLdTpGFlhMn","colab_type":"code","outputId":"8ceacdd1-391e-483a-cd04-e48c7e69017f","executionInfo":{"status":"ok","timestamp":1563427549823,"user_tz":240,"elapsed":241292,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["# Prediccion modelo\n","dataY_pred = model.predict_classes(X_test,verbose=0)\n","y_pred = [dataY_pred[i][-len(y_true[i]):] for i in range(len(dataY_pred))]\n","\n","# labels unicos en cada set.\n","uniq_true = set()\n","uniq_pred = set()\n","for i in range(len(y_pred)):\n","  if len(y_pred[i]) != len(y_true[i]):\n","    print(\"ERROR {}, {} - {}\".format(i, len(dataY_pred[i]), len(dataY[i])))\n","  for j in range(len(y_true[i])):\n","    uniq_true.add(y_true[i][j])\n","    uniq_pred.add(y_pred[i][j])\n","\n","print(\"labels in y_true: {}\".format(uniq_true))\n","print(\"labels in y_pred: {}\".format(uniq_pred))\n","print(\"----\\n\")\n","\n","#F1 score\n","f1_score_bydata = [f1_score(true, pred ,average='macro') for true, pred in zip(y_true, y_pred) ]\n","print(\"\\n\\nF1 score on test: \", np.mean(f1_score_bydata) )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["labels in y_true: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n","labels in y_pred: {0, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14, 17}\n","----\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n","  'recall', 'true', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","F1 score on test:  0.8339001960907853\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ODEVRZtkNLZ_","colab_type":"text"},"source":["## i) Red GRU"]},{"cell_type":"markdown","metadata":{"id":"yEg7TTLYlv6c","colab_type":"text"},"source":["### Modelo\n","\n","En el siguiente modelo se cambian las celdas LSTM por celdas GRU que tienen menos parámetros, pues pasa de tres compuertas en LSTM a sólo dos. Se entrena el nuevo modelo bajo dos configuraciones, primero sin entrenar la capa de _embeddings_ y, luego, entrenando los pesos de esta capa. Para ambos casos se inician los pesos con los encontrados con `Word2Vec`.\n","\n","El modelo sin entrenar el _embedding_ tiene 41.718 parámetros entrenables, 24% menos que el modelo equivalente con las compuertas LSTM. Este modelo demora cerca de tres minutos y medios en ser entrenado. En general, 30 segundos menos que el primer modelo\n","\n","Por otro lado, en el modelo que entrena la capa de _embedding_ se tienen 498.934 parámetros entrenables, de los cuales el 92% corresponde al _embedding_. Este modelo tarda cerca de cuatro minutos y medios en ser entrenado."]},{"cell_type":"code","metadata":{"id":"vFwQAHvrlxfS","colab_type":"code","outputId":"1e4387b1-8992-4679-ecc8-86a7bb4ddcc3","executionInfo":{"status":"ok","timestamp":1563428418602,"user_tz":240,"elapsed":213089,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":851}},"source":["from keras.layers import GRU\n","\n","# Modelo sin entrenar embedding\n","model = Sequential()\n","model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_input_lenght,\n","                    trainable=False, weights = [embedding_matrix]))\n","model.add(GRU(units=100,return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_labels, activation='softmax')))\n","model.summary()\n","print()\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, 70, 32)            457216    \n","_________________________________________________________________\n","gru_2 (GRU)                  (None, 70, 100)           39900     \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 70, 100)           0         \n","_________________________________________________________________\n","time_distributed_5 (TimeDist (None, 70, 18)            1818      \n","=================================================================\n","Total params: 498,934\n","Trainable params: 41,718\n","Non-trainable params: 457,216\n","_________________________________________________________________\n","\n","Train on 18000 samples, validate on 6000 samples\n","Epoch 1/15\n","18000/18000 [==============================] - 15s 848us/step - loss: 0.7663 - val_loss: 0.2616\n","Epoch 2/15\n","18000/18000 [==============================] - 14s 793us/step - loss: 0.2222 - val_loss: 0.1841\n","Epoch 3/15\n","18000/18000 [==============================] - 14s 792us/step - loss: 0.1694 - val_loss: 0.1501\n","Epoch 4/15\n","18000/18000 [==============================] - 14s 781us/step - loss: 0.1445 - val_loss: 0.1316\n","Epoch 5/15\n","18000/18000 [==============================] - 14s 773us/step - loss: 0.1297 - val_loss: 0.1200\n","Epoch 6/15\n","18000/18000 [==============================] - 14s 776us/step - loss: 0.1200 - val_loss: 0.1123\n","Epoch 7/15\n","18000/18000 [==============================] - 14s 766us/step - loss: 0.1128 - val_loss: 0.1057\n","Epoch 8/15\n","18000/18000 [==============================] - 14s 779us/step - loss: 0.1074 - val_loss: 0.1015\n","Epoch 9/15\n","18000/18000 [==============================] - 14s 776us/step - loss: 0.1030 - val_loss: 0.0978\n","Epoch 10/15\n","18000/18000 [==============================] - 14s 775us/step - loss: 0.0995 - val_loss: 0.0943\n","Epoch 11/15\n","18000/18000 [==============================] - 14s 767us/step - loss: 0.0963 - val_loss: 0.0914\n","Epoch 12/15\n","18000/18000 [==============================] - 14s 772us/step - loss: 0.0935 - val_loss: 0.0892\n","Epoch 13/15\n","18000/18000 [==============================] - 14s 770us/step - loss: 0.0909 - val_loss: 0.0865\n","Epoch 14/15\n","18000/18000 [==============================] - 14s 764us/step - loss: 0.0884 - val_loss: 0.0846\n","Epoch 15/15\n","18000/18000 [==============================] - 14s 764us/step - loss: 0.0866 - val_loss: 0.0829\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f81398f7eb8>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"XsVTJzrPUxm7","colab_type":"code","outputId":"8d497d8d-81c3-4f24-f252-94474cb3d147","executionInfo":{"status":"ok","timestamp":1563428635949,"user_tz":240,"elapsed":274034,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":851}},"source":["#Modelo entrenando pesos embedding\n","\n","model2 = Sequential()\n","model2.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_input_lenght,\n","                    trainable=True, weights = [embedding_matrix]))\n","model2.add(GRU(units=100,return_sequences=True))\n","model2.add(Dropout(0.2))\n","model2.add(TimeDistributed(Dense(n_labels, activation='softmax')))\n","model2.summary()\n","print()\n","\n","model2.compile(loss='categorical_crossentropy', optimizer='adam')\n","model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, 70, 32)            457216    \n","_________________________________________________________________\n","gru_3 (GRU)                  (None, 70, 100)           39900     \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 70, 100)           0         \n","_________________________________________________________________\n","time_distributed_6 (TimeDist (None, 70, 18)            1818      \n","=================================================================\n","Total params: 498,934\n","Trainable params: 498,934\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Train on 18000 samples, validate on 6000 samples\n","Epoch 1/15\n","18000/18000 [==============================] - 16s 892us/step - loss: 0.6904 - val_loss: 0.2090\n","Epoch 2/15\n","18000/18000 [==============================] - 15s 812us/step - loss: 0.1602 - val_loss: 0.1168\n","Epoch 3/15\n","18000/18000 [==============================] - 14s 789us/step - loss: 0.0971 - val_loss: 0.0795\n","Epoch 4/15\n","18000/18000 [==============================] - 14s 800us/step - loss: 0.0706 - val_loss: 0.0642\n","Epoch 5/15\n","18000/18000 [==============================] - 14s 797us/step - loss: 0.0587 - val_loss: 0.0576\n","Epoch 6/15\n","18000/18000 [==============================] - 14s 802us/step - loss: 0.0522 - val_loss: 0.0532\n","Epoch 7/15\n","18000/18000 [==============================] - 14s 797us/step - loss: 0.0476 - val_loss: 0.0505\n","Epoch 8/15\n","18000/18000 [==============================] - 14s 792us/step - loss: 0.0439 - val_loss: 0.0484\n","Epoch 9/15\n","18000/18000 [==============================] - 14s 797us/step - loss: 0.0414 - val_loss: 0.0474\n","Epoch 10/15\n","18000/18000 [==============================] - 14s 785us/step - loss: 0.0390 - val_loss: 0.0459\n","Epoch 11/15\n","18000/18000 [==============================] - 14s 771us/step - loss: 0.0370 - val_loss: 0.0450\n","Epoch 12/15\n","18000/18000 [==============================] - 14s 773us/step - loss: 0.0355 - val_loss: 0.0450\n","Epoch 13/15\n","18000/18000 [==============================] - 14s 777us/step - loss: 0.0339 - val_loss: 0.0441\n","Epoch 14/15\n","18000/18000 [==============================] - 14s 795us/step - loss: 0.0325 - val_loss: 0.0443\n","Epoch 15/15\n","18000/18000 [==============================] - 14s 789us/step - loss: 0.0312 - val_loss: 0.0431\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f813868eda0>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"qqmXKjXTlx8F","colab_type":"text"},"source":["### Evaluación\n","\n","Se obtiene una situación parecida a la con las celdas LSTM. El modelo sin entrenar _embeddings_ logra un puntaje _f1-score_ de $0.715$ y el modelo con entrenamiento obtiene $0.837$ puntos, dando una diferencia de $0.12$ puntos entre ambos. El modelo con entrenamiento predice 13 tags, mientras que el modelo que no entrena los _embeddings_ predice 10 de los 17 tags que existen en el dataset. Exceptuando que estos modelos tienen menos parámetros y son levementes más rápidos de entrenar, no se encuentra mucha diferencia con el desempeño obtenido por los modelos."]},{"cell_type":"code","metadata":{"id":"Wl9O7iSCNLaB","colab_type":"code","outputId":"f84a73c9-bad1-4114-ed90-5f071f8edd8a","executionInfo":{"status":"ok","timestamp":1563428705825,"user_tz":240,"elapsed":19671,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":280}},"source":["# Prediccion modelos\n","# modelo sin entrenar embedding\n","dataY_pred1 = model.predict_classes(X_test,verbose=0)\n","y_pred1 = [dataY_pred1[i][-len(y_true[i]):] for i in range(len(dataY_pred1))]\n","# modelo entrenando embeddings\n","dataY_pred2 = model2.predict_classes(X_test,verbose=0)\n","y_pred2 = [dataY_pred2[i][-len(y_true[i]):] for i in range(len(dataY_pred2))]\n","del model2\n","\n","# labels unicos en cada set.\n","uniq_true = set()\n","uniq_pred1 = set()\n","uniq_pred2 = set()\n","for i in range(len(y_pred)):\n","  for j in range(len(y_true[i])):\n","    uniq_true.add(y_true[i][j])\n","    uniq_pred1.add(y_pred1[i][j])\n","    uniq_pred2.add(y_pred2[i][j])\n","\n","print(\"Labels unicos encontrados\")\n","print(\"       labels in y_true: {}\".format(uniq_true))\n","print(\"Sin entrenar embeddings: {}\".format(uniq_pred1))\n","print(\"  Entrenando embeddings: {}\".format(uniq_pred2))\n","print(\"----\\n\")\n","\n","#F1 score\n","f1_score_1 = [f1_score(true, pred ,average='macro') for true, pred in zip(y_true, y_pred1) ]\n","f1_score_2 = [f1_score(true, pred ,average='macro') for true, pred in zip(y_true, y_pred2) ]\n","print(\"\\n\\nF1 score (sin entrenar): \", np.mean(f1_score_1) )\n","print(\"F1 score (con entrenar): \", np.mean(f1_score_2) )\n","del f1_score_1, f1_score_2"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Labels unicos encontrados\n","       labels in y_true: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n","Sin entrenar embeddings: {0, 2, 3, 4, 8, 9, 10, 11, 12, 13, 17}\n","  Entrenando embeddings: {0, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 17}\n","----\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n","  'recall', 'true', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","F1 score (sin entrenar):  0.7155913126002403\n","F1 score (con entrenar):  0.8372081189300733\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uFNHACQaNLaH","colab_type":"text"},"source":["## j) Red Neuronal Recurrente Bidireccional"]},{"cell_type":"markdown","metadata":{"id":"A3cBNHnLw2Bp","colab_type":"text"},"source":["### Modelo\n","\n","Este modelo usa una red recurrente bidireccional, estas redes son útiles cuando se tiene una tarea en la que es importante el contexto del input (lo que vino antes y lo que viene después) como es en el caso del presente problema sobre _named-entity recognition_. El _output_ de la recurrencia en sentido positivo del tiempo no está conectado al _output_ de la recurrencia en el sentido contrario, por lo que se debe definir una forma de \"combinar\" o unir ambas salidas para poder pasarlas como entrada a la capa densa. La implementación en `Keras` permite cuatro formas de unir las salidas de ambas recurrencias. `sum`, que suma elemento a elemento las salidas; `mul`, que las multiplica elemento a elemento; `avg`, que promedia elemento a elemento y, `concat`, que concatena ambos vectores. \n","\n","Para este problema se estima que la mejor opción es concatenar las salidas para preservar información del orden del contexto (antes o después del input). Por ejemplo, en casos con dos entidades juntas, del mismo tipo o diferente. Cuando entre como input la primera entidad, la recurrencia con sentido positivo tendrá información de los lemas que preceden que posiblemente no pertenescan a una entidad de interes y la recurrencia con sentido contrarió tendrá información de que viene un lema perteneciente a una entidad de interés. Si se suma, promedia o multiplica estas salidas, se va a seguir teniendo información de que en el contexto del input se tiene otra entidad de interés, pero se perderá el orden en la información. En cambio, al concatenar, se conserva este orden y la red puede aprender sobre el orden del contexto según en qué parte del vector esté la información.\n","\n","Ya que se decidió concatenar las salidas de ambas capas recurrentes, el vector de entrada de la capa densa final tiene el doble de datos, por lo que en esta capa se tiene el doble de pesos pasando de 1.800 a 3.600. Además, se tiene dos capas GRU, por lo que este modelo tiene ele doble de pesos en la parte recurrente. El entrenamiento tarda cerca de siete minutos y medios en ser completado"]},{"cell_type":"code","metadata":{"id":"-B-iJLkkw9PK","colab_type":"code","outputId":"60651059-567d-45c2-dbd6-bdb177efef12","executionInfo":{"status":"ok","timestamp":1563471812581,"user_tz":240,"elapsed":1551,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":278}},"source":["from keras.layers import Bidirectional\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_input_lenght,\n","                    trainable=True, weights = [embedding_matrix]))\n","model.add(Bidirectional(GRU(units=100,return_sequences=True), \n","                        merge_mode='concat'))\n","model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_labels, activation='softmax')))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 70, 32)            457216    \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 70, 200)           79800     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 70, 200)           0         \n","_________________________________________________________________\n","time_distributed_3 (TimeDist (None, 70, 18)            3618      \n","=================================================================\n","Total params: 540,634\n","Trainable params: 540,634\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4uU6GQlVxtuz","colab_type":"code","outputId":"161ff5ea-e3a5-4577-96ee-33f0a9e8dbea","executionInfo":{"status":"ok","timestamp":1563472298706,"user_tz":240,"elapsed":449712,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":573}},"source":["# Entrenamiento\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 18000 samples, validate on 6000 samples\n","Epoch 1/15\n","18000/18000 [==============================] - 32s 2ms/step - loss: 0.5411 - val_loss: 0.1572\n","Epoch 2/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.1125 - val_loss: 0.0803\n","Epoch 3/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0694 - val_loss: 0.0616\n","Epoch 4/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0552 - val_loss: 0.0537\n","Epoch 5/15\n","18000/18000 [==============================] - 29s 2ms/step - loss: 0.0471 - val_loss: 0.0481\n","Epoch 6/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0413 - val_loss: 0.0447\n","Epoch 7/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0368 - val_loss: 0.0423\n","Epoch 8/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0338 - val_loss: 0.0411\n","Epoch 9/15\n","18000/18000 [==============================] - 29s 2ms/step - loss: 0.0312 - val_loss: 0.0394\n","Epoch 10/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0286 - val_loss: 0.0387\n","Epoch 11/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0266 - val_loss: 0.0376\n","Epoch 12/15\n","18000/18000 [==============================] - 30s 2ms/step - loss: 0.0249 - val_loss: 0.0371\n","Epoch 13/15\n","18000/18000 [==============================] - 29s 2ms/step - loss: 0.0231 - val_loss: 0.0370\n","Epoch 14/15\n","18000/18000 [==============================] - 29s 2ms/step - loss: 0.0216 - val_loss: 0.0368\n","Epoch 15/15\n","18000/18000 [==============================] - 31s 2ms/step - loss: 0.0202 - val_loss: 0.0366\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fccdf4672e8>"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"br_FG_S2xzzZ","colab_type":"text"},"source":["### Evaluación\n","\n","Este modelo tiene mejor desempeño que los modelos anteriores, haciendo predicciones para los 17 tags que existen, aunque aún se sigue agregando el tag de _pre-padding_ demás. El puntaje _f1-score_ de $0.866$ es el más alto también, aunque con una diferencia de sólo $0.03$ puntos del que le sigue (modelo GRU entrenando _embeddings_)."]},{"cell_type":"code","metadata":{"id":"kloBM7xgx2em","colab_type":"code","outputId":"0b628fc3-90d5-43d6-ab77-95fd525b7ebe","executionInfo":{"status":"ok","timestamp":1563472314372,"user_tz":240,"elapsed":403705,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["# Prediccion modelo\n","dataY_pred = model.predict_classes(X_test,verbose=0)\n","y_pred = [dataY_pred[i][-len(y_true[i]):] for i in range(len(dataY_pred))]\n","\n","# labels unicos en cada set.\n","uniq_true = set()\n","uniq_pred = set()\n","for i in range(len(y_pred)):\n","  if len(y_pred[i]) != len(y_true[i]):\n","    print(\"ERROR {}, {} - {}\".format(i, len(dataY_pred[i]), len(dataY[i])))\n","  for j in range(len(y_true[i])):\n","    uniq_true.add(y_true[i][j])\n","    uniq_pred.add(y_pred[i][j])\n","\n","print(\"labels in y_true: {}\".format(uniq_true))\n","print(\"labels in y_pred: {}\".format(uniq_pred))\n","print(\"----\\n\")\n","\n","#F1 score\n","f1_score_bydata = [f1_score(true, pred ,average='macro') for true, pred in zip(y_true, y_pred)]\n","print(\"\\n\\nF1 score on test: \", np.mean(f1_score_bydata))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["labels in y_true: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n","labels in y_pred: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}\n","----\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n","  'recall', 'true', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","F1 score on test:  0.866107701963719\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a_tkSiHlNLaP","colab_type":"text"},"source":["## k) Mejoras al modelo bidireccional.\n","\n","Entre las opciones que existen para modificar el modelo, como usar otras celdas recurrentes (`keras` tiene implementado `ConvLSTM2D` que realiza una transformación convolucional en el _embedding_ y en la parte recurrente, que tiene mejores resultados con secuencias de datos 2D como un video). Se estima que  modificar el largo de las secuencias puede tener un buen efecto en la red.\n","\n","Como ya se observo en la sección b), el largo máximo de las secuencias es de 70, pero la gran mayoría de las secuencias tiene largo menor a 35. Es decir, una parte importante de las secuencias estará formada más por _padding_ que por datos. Es posible que esto tenga efectos negativos en el aprendizaje, pues se están procesando muchos datos que no sirven.\n","\n","Para disminuir el _padding_ se estudian dos opciones. La primera, consiste en no realizar _padding_ alguno, con esto se debe modificar el modelo para que no espere secuencias de largo fijo, además, de que el input se debe agrupar en batches distintos según el largo de las secuencias. En la segunda opción, se decide cortar las secuencias de largo mayor a 40 y hacer _pre-padding_ para dejarlas todas de largo fijo en 40. No se estudia el efecto de hacer _post-padding_ pues, basandonos en el paper \"Effects of padding on LSTMs and CNNs\" [[1]](#refs), el desempeño de los modelos es menor."]},{"cell_type":"markdown","metadata":{"id":"48GjqEoRsTho","colab_type":"text"},"source":["### Opción 1: Sin _padding_\n","\n","Con este modelo se quiere observar si se mejoran los resultados al no utilizar _padding_ en las secuencias. Se debe modificar el modelo para que no espere secuencias de largo fijo, para esto se usa el parámetro `input_length=None` en la primera capa.\n","\n","Por la forma en que está implementado `keras`, la red sólo acepta como input batches cuyos elementos tengan el mismo largo. Es decir, todas las secuencias dentro de un batch deben tener el mismo largo, pero el largo de las secuencias sí puede variar entre batch y batch. Asimismo, la cantidad de elementos por batch también puede variar. Para agrupar los elementos según su largo y suministrar estos batches al modelo, se construye una clase con base en `keras.utils.Sequence`.\n","\n","El tamaño de cada batch depende de la cantidad de secuencias que existan con determinado largo. Así, batch con secuencias muy cortas (largo < 10) y secuencias muy largas (> 40) tendrán muy pocos elementos. Incluso se tendrán batches con un único elemento. Además, se especifica un tope máximo a la cantidad de elementos de un batch por medio del parámetro `max_size = 128`, por lo que se tendran batch con tamaños entre 1 y 128 elementos para entrenar el modelo.\n","\n","\n","El entrenamiento del modelo, dura un poco menos de tres minutos. Siendo el modelo más rápido en ser entrendado, pues la cantidad de datos que debe procesar es mucho menor al no tener _padding_. Se puede observar, también, que la _loss_ en el conjunto de entrenamiento alcanza un valor relativamente bajo de $0.0484$, pero la _loss_ en el conjunto de validación se mantiene más alto llegando a un valor de $0.1143$ en el último _epoch_. Mostrando ya, que la modificación hecha no resulta en una mejora del modelo, incluso, podría inducir a _overfitting_.\n","\n","Al evaluar el modelo con el set de testing, se obtiene que, si bien el modelo utiliza todos los tags en sus predicciones (a diferencia de los primeros cuatro modelos), se logra un puntaje de _f1-score_ de $0.676$ siendo el más bajo de todos los modelos.\n","\n"]},{"cell_type":"code","metadata":{"id":"1-DmQ4AfxSg7","colab_type":"code","colab":{}},"source":["from keras.utils import Sequence\n","\n","class DataSeq(Sequence):\n","  \"\"\" Agrupa secuencias en batches segun largo.\n","  \n","  Cada batch tiene tamaño de entre 1 a max_size elementos.\n","  \"\"\"\n","  \n","  def __init__(self, x_set, y_set, max_size):\n","    self.x = []\n","    self.y = []\n","    aux = pd.Series([len(x) for x in x_set])\n","    seq_len = pd.Series.groupby(aux, by=aux).indices\n","    for k in seq_len.keys():\n","      bx = []\n","      by = []\n","      for ix in seq_len[k]:\n","        bx.append(x_set[ix])\n","        by.append(y_set[ix])\n","        if len(bx)==max_size:\n","          self.x.append(np.array(bx))\n","          self.y.append(np.array(by))\n","          bx = []\n","          by = []\n","      if len(bx)>0:\n","        self.x.append(np.array(bx))\n","        self.y.append(np.array(by))\n","  \n","  def __len__(self):\n","    return len(self.x)\n","  \n","  def __getitem__(self, idx):\n","    return self.x[idx], self.y[idx]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuOewrjW44Qc","colab_type":"code","colab":{}},"source":["dataY = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY_raw]\n","dataX = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX_raw]\n","dataY_cat = np.array([to_categorical(i, num_classes=n_labels-1) for i in dataY])\n","X_aux, X_test, y_aux, y_test = train_test_split(dataX, dataY_cat, test_size=0.2, random_state=22)\n","X_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, test_size=0.25, random_state=22)\n","_, y_true = train_test_split(dataY, test_size=0.2, random_state=22)\n","del dataX[:], dataY[:]\n","del y_aux, X_aux, dataY_cat, dataX, dataY"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZdu4mk2Xfy0","colab_type":"code","colab":{}},"source":["# INFO Nuevo embeddings con todos los lemas\n","# INFO descartado por que empeora los resultados\n","EMBEDDING_DIM = 32\n","window_size = 5\n","nb_epoch = 5\n","batch_size = 6000\n","min_count = 0 # Para usar todos los lemas\n","w2v_model = Word2Vec(dataX_raw, size=EMBEDDING_DIM, window=window_size, \n","                 batch_words=batch_size, iter=nb_epoch, min_count=min_count, \n","                 negative=5, sg=1) #sg=1 mean skip-gram"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbRGEo2RwXlG","colab_type":"code","outputId":"da703fd3-a3a4-49eb-aea9-d1a558a38d37","executionInfo":{"status":"ok","timestamp":1563472668119,"user_tz":240,"elapsed":175844,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":833}},"source":["model = Sequential()\n","# model.add(w2v_model.wv.get_keras_embedding()) # nueva capa embeddings\n","model.add(Embedding(input_dim=n_lemmas-1, output_dim=EMBEDDING_DIM, input_length=None, trainable=True, weights = [embedding_matrix[:-1]]))\n","model.add(Bidirectional(GRU(units=100, return_sequences=True), merge_mode='concat'))\n","model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_labels-1, activation='softmax')))\n","model.summary()\n","print()\n","\n","# Entrenamiento\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=1)\n","train_seq = DataSeq(X_train, y_train, 128)\n","val_seq = DataSeq(X_val, y_val, 128)\n","model.fit_generator(train_seq, validation_data=val_seq, epochs=15)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, None, 32)          457184    \n","_________________________________________________________________\n","bidirectional_5 (Bidirection (None, None, 200)         79800     \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, None, 200)         0         \n","_________________________________________________________________\n","time_distributed_5 (TimeDist (None, None, 17)          3417      \n","=================================================================\n","Total params: 540,401\n","Trainable params: 540,401\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Epoch 1/15\n","180/180 [==============================] - 13s 73ms/step - loss: 0.6378 - val_loss: 0.3320\n","Epoch 2/15\n","180/180 [==============================] - 12s 65ms/step - loss: 0.2611 - val_loss: 0.2041\n","Epoch 3/15\n","180/180 [==============================] - 11s 63ms/step - loss: 0.1822 - val_loss: 0.1734\n","Epoch 4/15\n","180/180 [==============================] - 11s 63ms/step - loss: 0.1479 - val_loss: 0.1516\n","Epoch 5/15\n","180/180 [==============================] - 12s 66ms/step - loss: 0.1252 - val_loss: 0.1453\n","Epoch 6/15\n","180/180 [==============================] - 11s 61ms/step - loss: 0.1109 - val_loss: 0.1352\n","Epoch 7/15\n","180/180 [==============================] - 11s 61ms/step - loss: 0.0993 - val_loss: 0.1293\n","Epoch 8/15\n","180/180 [==============================] - 11s 63ms/step - loss: 0.0899 - val_loss: 0.1228\n","Epoch 9/15\n","180/180 [==============================] - 11s 63ms/step - loss: 0.0818 - val_loss: 0.1213\n","Epoch 10/15\n","180/180 [==============================] - 11s 62ms/step - loss: 0.0752 - val_loss: 0.1167\n","Epoch 11/15\n","180/180 [==============================] - 11s 62ms/step - loss: 0.0689 - val_loss: 0.1149\n","Epoch 12/15\n","180/180 [==============================] - 11s 62ms/step - loss: 0.0639 - val_loss: 0.1145\n","Epoch 13/15\n","180/180 [==============================] - 11s 63ms/step - loss: 0.0582 - val_loss: 0.1186\n","Epoch 14/15\n","180/180 [==============================] - 11s 62ms/step - loss: 0.0538 - val_loss: 0.1119\n","Epoch 15/15\n","180/180 [==============================] - 11s 63ms/step - loss: 0.0484 - val_loss: 0.1143\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fcc6da0f588>"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"0OH7K6-ophJS","colab_type":"code","outputId":"a5ec1e3b-2ff5-4102-8775-3635bf7b4cd4","executionInfo":{"status":"ok","timestamp":1563472734674,"user_tz":240,"elapsed":14914,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["# Prediccion modelo\n","y_pred = []\n","for x in X_test:\n","  y_pred.append(model.predict_classes(x,verbose=0).ravel())\n","#y_pred = [dataY_pred[i][-len(y_true[i]):] for i in range(len(dataY_pred))]\n","\n","# labels unicos en cada set.\n","uniq_true = set()\n","uniq_pred = set()\n","for i in range(len(y_pred)):\n","  if len(y_pred[i]) != len(y_true[i]):\n","    print(\"ERROR {}, {} - {}\".format(i, len(dataY_pred[i]), len(dataY[i])))\n","  for j in range(len(y_true[i])):\n","    uniq_true.add(y_true[i][j])\n","    uniq_pred.add(y_pred[i][j])\n","\n","print(\"labels in y_true: {}\".format(uniq_true))\n","print(\"labels in y_pred: {}\".format(uniq_pred))\n","print(\"----\\n\")\n","\n","#F1 score\n","f1_score_bydata = [f1_score(true, pred ,average='macro') for true, pred in zip(y_true, y_pred)]\n","print(\"\\n\\nF1 score on test: \", np.mean(f1_score_bydata))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["labels in y_true: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n","labels in y_pred: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n","----\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n","  'recall', 'true', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","F1 score on test:  0.6764866471470514\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SEJXQqK6yCow","colab_type":"text"},"source":["### Opción 2: Dividir secuencias\n","\n","En esta parte se busca reducir el largo máximo de las secuencias a 40. Una alternativa para esto, es truncar todas las secuencias de largo mayor a 40. Sin embargo, se estima que puede desperdiciar información con las frases muy largas. Por esto, se prueba dividiendo por la mitad las frases de largo mayor a 40. Luego, se hace _pre-padding_ para dejar todas las secuencias de largo fijo en 40.\n","\n","El modelo es el mismo al de la red bi-direccional de la sección j), pero especificando el parámetro `input_length=40`. El modelo tarda cerca de cuatro minutos y medios en ser entrenado y la discrepancia entre la _loss_ de entrenamiento y la del set de validación es bastante menor ($0.03$ puntos).\n","\n","En la evaluación del modelo se observa una mejora en el _f1-score_, logrando un valor de $0.87$, auemtando en $0.04$ puntos con respecto al mejor puntaje logrado anteriormente en la sección j). Este modelo, también utiliza todos los tags en las predicciones pero sigue usando demás el tag del token _pre-padding_."]},{"cell_type":"code","metadata":{"id":"mFD-fg3i2x_2","colab_type":"code","colab":{}},"source":["def cutSeq(x_set, y_set, max_len=40):\n","  \"\"\" Dividie por la mitad secuencias de largo mayor a max_len\n","  \"\"\"\n","  new_x = []\n","  new_y = []\n","  for i in range(len(x_set)):\n","    if len(x_set[i])<=max_len:\n","      new_x.append(x_set[i])\n","      new_y.append(y_set[i])\n","    else:\n","      m = int(len(x_set[i])/2)\n","      if y_set[0][:m][-1] != lab2idx[\"O\"]:\n","        m = m+2\n","        print(\"[{}] m-upd: {}\\n  {}\\n  \".format(i, m, y_set[:m-3], y_set[:m]))\n","      new_x.append(x_set[i][:m])\n","      new_x.append(x_set[i][m:])\n","      new_y.append(y_set[i][:m])\n","      new_y.append(y_set[i][m:])\n","  return new_x, new_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9U5H29iQCj1L","colab_type":"code","outputId":"11df155f-8fa8-41f1-9f9e-5feba0c1b2f4","executionInfo":{"status":"ok","timestamp":1563472801255,"user_tz":240,"elapsed":1867,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["dataY_aux = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY_raw]\n","dataX_aux = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX_raw]\n","dataX, dataY = cutSeq(dataX_aux, dataY_aux)\n","\n","max_len = max([len(x) for x in dataX])\n","X = sequence.pad_sequences(dataX, maxlen=max_len, padding='pre', value=lemma2idx[\"_PRE-PADDING_\"])\n","y = sequence.pad_sequences(dataY, maxlen=max_len, padding='pre', value=lab2idx[\"_PRE-PADDING_\"])\n","\n","y_cat = np.array([to_categorical(i, num_classes=n_labels) for i in y])\n","\n","X_aux, X_test, y_aux, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=22)\n","X_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, test_size=0.25, random_state=22)\n","_, y_true = train_test_split(dataY, test_size=0.2, random_state=22)\n","\n","del dataX_aux[:], dataY_aux[:], dataX[:], dataY[:]\n","del y_aux, X_aux, y_cat, dataX_aux, dataY_aux, dataX, dataY, X, y\n","\n","print(\"Cjto entrenamiento:\\n   X = {}\\n   Y = {}\\n\".format(X_train.shape, y_train.shape))\n","print(\"Cjto validación/prueba:\\n   X = {}\\n   Y = {}\\n\".format(X_val.shape, y_val.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cjto entrenamiento:\n","   X = (18264, 40)\n","   Y = (18264, 40, 18)\n","\n","Cjto validación/prueba:\n","   X = (6088, 40)\n","   Y = (6088, 40, 18)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HnNBrjZ3GYJe","colab_type":"code","outputId":"2f0fe61d-5086-4a23-d9a5-9580efd80b3a","executionInfo":{"status":"ok","timestamp":1563473076472,"user_tz":240,"elapsed":276314,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":851}},"source":["model = Sequential()\n","model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_len,\n","                    trainable=True, weights = [embedding_matrix]))\n","model.add(Bidirectional(GRU(units=100,return_sequences=True), \n","                        merge_mode='concat'))\n","model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_labels, activation='softmax')))\n","model.summary()\n","print()\n","\n","# Entrenamiento\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, 40, 32)            457216    \n","_________________________________________________________________\n","bidirectional_6 (Bidirection (None, 40, 200)           79800     \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 40, 200)           0         \n","_________________________________________________________________\n","time_distributed_6 (TimeDist (None, 40, 18)            3618      \n","=================================================================\n","Total params: 540,634\n","Trainable params: 540,634\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Train on 18264 samples, validate on 6088 samples\n","Epoch 1/15\n","18264/18264 [==============================] - 20s 1ms/step - loss: 0.6422 - val_loss: 0.2319\n","Epoch 2/15\n","18264/18264 [==============================] - 18s 986us/step - loss: 0.1686 - val_loss: 0.1236\n","Epoch 3/15\n","18264/18264 [==============================] - 18s 985us/step - loss: 0.1087 - val_loss: 0.0987\n","Epoch 4/15\n","18264/18264 [==============================] - 18s 994us/step - loss: 0.0872 - val_loss: 0.0861\n","Epoch 5/15\n","18264/18264 [==============================] - 18s 976us/step - loss: 0.0747 - val_loss: 0.0787\n","Epoch 6/15\n","18264/18264 [==============================] - 18s 986us/step - loss: 0.0662 - val_loss: 0.0743\n","Epoch 7/15\n","18264/18264 [==============================] - 18s 990us/step - loss: 0.0601 - val_loss: 0.0704\n","Epoch 8/15\n","18264/18264 [==============================] - 18s 964us/step - loss: 0.0550 - val_loss: 0.0684\n","Epoch 9/15\n","18264/18264 [==============================] - 18s 978us/step - loss: 0.0507 - val_loss: 0.0664\n","Epoch 10/15\n","18264/18264 [==============================] - 18s 979us/step - loss: 0.0471 - val_loss: 0.0656\n","Epoch 11/15\n","18264/18264 [==============================] - 18s 990us/step - loss: 0.0436 - val_loss: 0.0640\n","Epoch 12/15\n","18264/18264 [==============================] - 18s 995us/step - loss: 0.0405 - val_loss: 0.0637\n","Epoch 13/15\n","18264/18264 [==============================] - 18s 992us/step - loss: 0.0379 - val_loss: 0.0631\n","Epoch 14/15\n","18264/18264 [==============================] - 18s 976us/step - loss: 0.0353 - val_loss: 0.0629\n","Epoch 15/15\n","18264/18264 [==============================] - 18s 978us/step - loss: 0.0330 - val_loss: 0.0631\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fcc63722898>"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"A62Wh6kiH0nW","colab_type":"code","outputId":"3b92ce14-7c2a-4596-c248-d69927fc5c31","executionInfo":{"status":"ok","timestamp":1563473087951,"user_tz":240,"elapsed":284221,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["# Prediccion modelo\n","dataY_pred = model.predict_classes(X_test,verbose=0)\n","y_pred = [dataY_pred[i][-len(y_true[i]):] for i in range(len(dataY_pred))]\n","\n","# labels unicos en cada set.\n","uniq_true = set()\n","uniq_pred = set()\n","for i in range(len(y_pred)):\n","  if len(y_pred[i]) != len(y_true[i]):\n","    print(\"ERROR {}, {} - {}\".format(i, len(dataY_pred[i]), len(dataY[i])))\n","  for j in range(len(y_true[i])):\n","    uniq_true.add(y_true[i][j])\n","    uniq_pred.add(y_pred[i][j])\n","\n","print(\"labels in y_true: {}\".format(uniq_true))\n","print(\"labels in y_pred: {}\".format(uniq_pred))\n","print(\"----\\n\")\n","\n","#F1 score\n","f1_score_bydata = [f1_score(true, pred ,average='macro') for true, pred in zip(y_true, y_pred)]\n","print(\"\\n\\nF1 score on test: \", np.mean(f1_score_bydata))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["labels in y_true: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n","labels in y_pred: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}\n","----\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n","  'recall', 'true', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","F1 score on test:  0.8704191233297411\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QrtBR8DUNLaY","colab_type":"text"},"source":["## l) Ejemplo de predicción\n","\n","Se utiliza la red de la segunda variación en la sección anterior, es decir, se usa la red entrenada con secuencias de largo fijo en 40. En el ejemplo utlizado más abajo, se muestran tres columnas: la primera, corresponde a los lemas de la frase; la segunda, a los tags predichos por la red y, la tercera, a los tags reales.\n","\n","Se observa que las entidades que predijo el modelo, las predijo correctamente. Sin embargo, el modelo parece tener problemas con entidades compuestas por más de un lema. Como es en el caso de `japan's` y `foreign ministri`, el modelo logra predecir correctamente sólo uno de los lemas que componen cada entidad (`japan` y `ministri` respectivamente) y el otro lema lo cataloga erroneamente con el tag `O`.\n","\n","El problema anterior se podría solucionar realizando un pre-procesamiento de los datos para identificar estas entidades formadas por dos o más tokens y unirlas en el dataset como un solo token. La librería `gensim` tiene la clase `gensim.models.phrases.Phrases` que es capaz de detectar bigramas, para construir trigramas y n-gramas, se debe usar nuevamente esta clase sobre el corpus actualizado con los bigramas y así sucesivamente.\n","\n","Otra modificación que puede ayudar en generar mejores modelos es adaptar la función de pérdida para que se considere mejor situaciones que se dan en problemas sobre _Named-entity recognition_. Por ejemplo, el caso de entidades compuestas por más de un lema, se puede adaptar la función para que al comparar las predicciones con los tags reales detecte cuando hay más de un tag especial contiguo para que asigne pesos especiales a estos casos. Otra situación especial es cuando el modelo predice que un lema pertenece a una entidad, pero se equivoca en el tipo de entidad (por ejemplo: asigna un tag `B-geo` cuando en verdad es `B-org`), la función de perdida se puede adaptar para que penalice distinto estos casos a otros tipos de errores como no identificar una entidad o identificar una entidad cuando no existe."]},{"cell_type":"code","metadata":{"id":"skuBVyn9NLaa","colab_type":"code","outputId":"4460e61a-094f-4492-e9cd-8cbe30fdca6d","executionInfo":{"status":"ok","timestamp":1563479019369,"user_tz":240,"elapsed":1905,"user":{"displayName":"Miguel Ibáñez Parraguez","photoUrl":"https://lh3.googleusercontent.com/-zYwCmWjZcXI/AAAAAAAAAAI/AAAAAAAAABo/EhL3gW62xCY/s64/photo.jpg","userId":"02140366121493912651"}},"colab":{"base_uri":"https://localhost:8080/","height":729}},"source":["l_lemmas = list(lemmas)\n","l_labels = list(labels)\n","l_lemmas.append(\"_PRE-PADDING_\")\n","l_labels.append(\"_PRE-PADDING_\")\n","\n","i = 1000\n","y_tr = sequence.pad_sequences([y_true[i]], maxlen=max_len, padding='pre', value=lab2idx[\"_PRE-PADDING_\"])[0]\n","p = model.predict(np.array([X_test[i]]))\n","p = np.argmax(p, axis=-1)\n","print(\"{:15}: {:17} ({})\".format(\"Lemma\", \"Pred\", \"true\"))\n","for w, pre, tru in zip(X_test[i], p[0], y_tr):\n","  print(\"{:15}: {:17} ({})\".format(l_lemmas[w], l_labels[pre], l_labels[tru]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Lemma          : Pred              (true)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","_PRE-PADDING_  : _PRE-PADDING_     (_PRE-PADDING_)\n","japan          : B-geo             (B-geo)\n","'s             : O                 (B-org)\n","foreign        : O                 (I-org)\n","ministri       : I-org             (I-org)\n","say            : O                 (O)\n","russian        : B-gpe             (B-gpe)\n","author         : O                 (O)\n","have           : O                 (O)\n","seiz           : O                 (O)\n","four           : O                 (O)\n","japanes        : B-gpe             (B-gpe)\n","fish           : O                 (O)\n","boat           : O                 (O)\n","in             : O                 (O)\n","disput         : O                 (O)\n","water          : O                 (O)\n","off            : O                 (O)\n","russia         : B-geo             (B-geo)\n","'s             : O                 (O)\n","far            : O                 (O)\n","east           : O                 (O)\n","on             : O                 (O)\n","suspicion      : O                 (O)\n","of             : O                 (O)\n","illeg          : O                 (O)\n","fish           : O                 (O)\n",".              : O                 (O)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pu6QI9FiL8Bo","colab_type":"text"},"source":["<a id=\"refs\"></a>\n","## Referencias\n","[1] Dwarampudi, M., & Reddy, N. V. (2019). Effects of padding on LSTMs and CNNs. arXiv preprint [arXiv:1903.07288](https://arxiv.org/abs/1903.07288)\n","\n","."]}]}